{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14dde600-a738-4e9b-91d9-0edae540f483",
   "metadata": {},
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>\n",
    "    <center>\n",
    "        <div style='  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;'>LCP-B - Processing of SEVN data for binary black holes mass distribution analysis\n",
    "        </div>\n",
    "    <center>\n",
    "        <br>\n",
    "    <i>Tommaso Bertola, Giacomo Di Prima, Giuseppe Viterbo, Marco Zenari</i></center>\n",
    "    <center>\n",
    "    <i>All authors contributed equally to the project</i></center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abe26b-8545-4bae-8bd7-a799da7478ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# The physical problem\n",
    "In the era of Gravitational Waves detector of the LIGO-Virgo-KAGRA collaboration sensible to $10^2$ Hz, the study of binary compact objects has become of extreme importance, since it can provide a landscape of the parameters space of the progenitor of these phenomena. The subject of our studies are binary black holes obtained by the population synthesis code SEVN <a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1), which interpolates stellar tracks to deal with stellar evolution, and it also takes into account supernova explosions and  binary evolution processes. In particular we aim to understand the importance of the input features of the program in guiding the evolution, and we also try to reproduce the final mass distribution, and some quantities correlated to it. Moreover we aim to define a possible region of the parameters space for which our model prediction could be use as proxy of the SEVN output, cutting the time of computation. \n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) Presentation papers:  <a href=\"https://ui.adsabs.harvard.edu/abs/2023MNRAS.tmp.1606I/abstract\">Iorio et al., 2022</a>, and  <a href=\"https://ui.adsabs.harvard.edu/abs/2019MNRAS.485..889S/abstract\">Spera et al., 2019 </a>.\n",
    "\n",
    "## Black holes and Binary evolution\n",
    "Before starting with the description of the algorithm that we use, it is important to understand what exatly are black holes, and how stars evolve into them.\n",
    "\n",
    "A black hole is a region of the spacetime so dense that nothing has energy high enough to escape from it.\n",
    "A black hole is formed when a Mass $M$ is within its Schwarzschild Radius $r_s$:\n",
    "\n",
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 200px; max-height: 50px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>\n",
    "    $r_s$ = $ \\frac{2GM}{c^2} \\approx 3 \\big(\\frac{M}{M_\\odot}\\big) \\text{km}$ \n",
    "</div></center>\n",
    "\n",
    "When a star ends its nuclear burning material, no sorce of energy can support the gravity pressure and the star goes out of the hydrostatic equilibrium<a name=\"cite_ref-2\"></a>[<sup>[2]</sup>](#cite_note-2), and the core collapse begins. Also, if the Mass of the star is bigger than $20 M_{\\odot}$ the gravity is so strong that no source of pressure can stop the core collpase, and a stellar mass black hole is formed. \n",
    "As we know from observations, almost all the massive stars ($M$ >  $20 M_{\\odot}$) are in binaries (or higher multiplicity systems), so it is important to understand the evolution of binary system; more specifically, SEVN runs are done with isolated binaries, which means that the two stars form from the same cloud and evolve in a gravitationally bound system.\n",
    "\n",
    "<a name=\"cite_note-2\"></a>2. [^](#cite_ref-2) Hydrostatic equilibrium is described as the balance between Pressure and Nuclear forces and Gravity.\n",
    "\n",
    "\n",
    "## Our main focus\n",
    "\n",
    "###  What we try to reproduce\n",
    "From the black hole mass ($M_{bh1}$, $M_{bh2}$ with $M_{bh1}$> $M_{bh2}$), the metallicity $Z$, the semimajor axis $a$, the eccentricity $e$ and the $\\alpha$ parameter, described below, we try to infere the distribution of two quantities: \n",
    "The Mass ratio $q$ of the black holes and the Chirp mass $\\mathcal{M}$. From an observational point the Chirp mass $\\mathcal{M}$ can be also obtain from Gravitational Wave frequency $f$, and its time derivative $\\dot{f}$. These two quantities are defined as:\n",
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 400px; max-height: 50px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>\n",
    "   $q = \\frac{M_{bh2}}{M_{bh1}}$ $\\quad$ $\\mathcal{M}$ = $ \\frac{(M_{bh1} M_{bh2})^{3/5}}{(M_{bh1} + M_{bh2})^{1/5}} = \\frac{c^3}{G} \\big( \\frac{5}{96} \\pi^{-8/3} f^{-11/3} \\dot{f} \\big)^{3/5} $ \n",
    "</div></center>\n",
    "\n",
    "\n",
    "### How supernovae influence our predictions\n",
    "The core collapse is one fondamental ingredient to define the final mass of a black hole. During the final stages of evolution the Silicon burning shell adds material to the Iron core, and when this one gets to a total mass bigger than the Chandrasekar Mass ($1.3-1.7 M_{\\odot}$), the degenerate electron pressure is not able to sustain anymore the gravity, and so the runaway core collapse begins. \n",
    "\n",
    "When the shock wave is not stopped the star explode as a Supernova, leaving a compact remant, either a Neutron star or a low-mass black hole. If the envelope binding energy is bigger than the Supernova energy the shock wave stops and there is no Supernova explosion, and the star collapses to a black hole of higher mass<a name=\"cite_ref-3\"></a>[<sup>[3]</sup>](#cite_note-3); this process is called Direct Collapse. In SEVN the model of the supernova can be described using different formalism and the one used in the data that we analyze is `rapid_gauNS`<a name=\"cite_ref-4\"></a>[<sup>[4]</sup>](#cite_note-4).\n",
    "\n",
    "When a Supernova occurs, asymmetry in mass ejection or in the neutrino emission, or symmetric mass loss in binary system<a name=\"cite_ref-5\"></a>[<sup>[5]</sup>](#cite_note-5), leads to the so called `Kick`, which is the gained momentum obtained by the compact object after the Supernova. Unfortunately for binary black holes supernova kicks there is no real observation. The kick velocity $v_k$ is extracted from a Maxwellian distribution and, to obtain the `effective` value $v_{ek}$<a name=\"cite_ref-4\"></a>[<sup>[4]</sup>](#cite_note-4), it is corrected by the fraction of matter $f_b$ that falls back on the proto compact object as follows:\n",
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 200px; max-height: 50px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>\n",
    "    $v_{ek}$ = $ v_k(1-f_b)$ \n",
    "</div></center>\n",
    "\n",
    "It is important to notice that when $f_b = 1$ a Direct Collpase is triggered, no mass is ejected, and then $v_{ek} = 0$, which means that there is no kick.\n",
    "\n",
    "\n",
    "Since the kick introduces a stochastic process in the evolution of the binary, in our algorithm we try to derive, a posteriori, how much this kind of phenomena  deviates our prediction of the output mass distribution.\n",
    "\n",
    "\n",
    "<a name=\"cite_note-3\"></a>3. [^](#cite_ref-3) <a href=\"https://ui.adsabs.harvard.edu/abs/2001ApJ...554..548F/abstract\"> Fryer C. and Kalogera V., 2001</a>\n",
    "\n",
    "<a name=\"cite_note-4\"></a>4. [^](#cite_ref-4) formalism described in<a href=\"https://iopscience.iop.org/article/10.1088/0004-637X/749/1/91\"> Fryer C. and et al., 2012</a>, but the mass of neutron star are drawn from a Gaussian\n",
    "\n",
    "<a name=\"cite_note-5\"></a>5. [^](#cite_ref-5) <a href=\"https://ui.adsabs.harvard.edu/abs/1961BAN....15..265B/abstract\"> Blaauw, A. , 1961</a>\n",
    "\n",
    "\n",
    "### How evolution is perturbed in a binary system \n",
    "Binary stars can exchange mass in a varity of channels. We investigate the number of Roche Lobe Overflow (RLO) and the number of Common Envelope (CE) that the data undergoes. These two kind of events are briefly described below.\n",
    "\n",
    "#### Roche Lobe Overflow (RLO)\n",
    "Assuming the binary orbit is circular, if we want to study the binary system using a non-inertial frame of reference that co-rotate with the binary (so that the stars position is fixed in time) we need to define the Roche Potential $\\phi_R$: \n",
    "\n",
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 250px; max-height: 50px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>\n",
    "    $\\phi_{R}$ = $ \\frac{G M_1}{|\\vec{r} - \\vec{d_1}|} + \\frac{G M_2}{|\\vec{r} - \\vec{d_2}|} + \\frac{1}{2} |\\vec{\\omega} \\times \\vec{r} |^2 $ \n",
    "</div></center>\n",
    "\n",
    "The Roche lobes are defined as the two lobes of the 8-shape critical potential surface, connected in the middle by the inner Lagrangian point L1. It is defined as `Roche Lobe Overflow` the flow of matter without energy change from the donor to the accretor, and it happens when the donor starts filling its Roche lobe (its radius is equal or larger than the Roche lobe radius). In SEVN the RLO stability is determined by the mass ratio of the two stars and by a parameter that depends on the evolution phase of the donor.\n",
    "If the RLO is unstable there are two possible outcomes:\n",
    "* if the stars do no have a core, they merge \n",
    "* if one of the two has a core, the stars enter in a `Common Envelope` phase\n",
    "\n",
    "\n",
    "#### Common Envelope (CE)\n",
    "As described before, when the RLO becomes unstable the envelope of the donor is shared between the remaing cores. Dragged by the envelope, the two cores start to spiral inwards (efficient way of shrinking a binary). This process can lead to the merging of the two core into one single star, or, if the energy released during the spiral removes the envelope, the two cores form a new tighter binary. In order to describe this phenomena, the $\\alpha$ formalism<a name=\"cite_ref-6\"></a>[<sup>[6]</sup>](#cite_note-6) is use by SEVN. The `alpha` parameter is the energy removal efficiency of the Common Envelope and is a free parameter that regulates variation of orbital energy of the cores, before and after the common envelope, $\\Delta E_{orb} = \\alpha (E_{orb, f} - E_{orb, i})$, where $E_{orb}$ is defined as:\n",
    "\n",
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 300px; max-height: 50px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>\n",
    "    $E_{orb, k}$ = $ \\frac{1}{2} \\frac{G M_{c,1} M_{c,2}}{a_k} \\quad  k \\in \\{ i, f\\}$\n",
    "</div></center> \n",
    "\n",
    "$M_{c,i}$ are the mass of the cores and $a_k$ is the semimajor axis of the cores binary.\n",
    "\n",
    "<a name=\"cite_note-6\"></a>6. [^](#cite_ref-6) <a href=\"https://ui.adsabs.harvard.edu/abs/2002MNRAS.329..897H/abstract\"> Hurley et al., 2002</a>\n",
    "\n",
    "\n",
    "## Parameters used for the algorithm\n",
    "For this reason we examin the following parameter space:\n",
    "\n",
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| [$M_0, M_1$]  | Initial masses of the stars in the binary |\n",
    "| $Z$         | Metallicity of the two starts in the binary |\n",
    "| $a$         | Semi-major axis of the binary               |\n",
    "| $e$         | Eccentricity of the binary                  |\n",
    "| $\\alpha$  | Energy removal efficency of the CE            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43794c-2302-4f75-9d51-ddbad97bfc13",
   "metadata": {},
   "source": [
    "# A brief recap on the preprocessing of the data\n",
    "Given the initial 1.7TB of data, we need to first preprocess it in order to extract only the meaningful information.\n",
    "To achieve this goal we use a cluster of 6 VMs hosted in CloudVeneto leveraging the capabilities of Python Dask library.\n",
    "\n",
    "We show below the implementation of the code used to parse the whole dataset. The data extracted from this algorithm is then saved onto disk to avoid rerunning the whole computation which takes about 2 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb9f44-3b16-4f09-adee-686ccfb7249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_{}.csv\n",
    "output_column_to_read = ['name', 'Mass_0', 'RemnantType_0',\n",
    "                         'Mass_1', 'RemnantType_1',\n",
    "                         'Semimajor','Eccentricity',\n",
    "                         'GWtime','BWorldtime']\n",
    "output_column_type = ['string', 'float64', 'int64',\n",
    "                      'float64', 'int64',\n",
    "                      'float64', 'float64',\n",
    "                      'float64', 'float64']\n",
    "\n",
    "# evolved_{}.dat\n",
    "evolved_column_to_read = ['name', 'Mass_0',\n",
    "                          'Z_0', 'SN_0',\n",
    "                          'Mass_1', 'SN_1',\n",
    "                          'a', 'e']\n",
    "evolved_column_type = ['string', 'float64',\n",
    "                       'float64', 'string',\n",
    "                       'float64', 'string',\n",
    "                       'float64', 'float64']\n",
    "\n",
    "# further columns to remove at the end \n",
    "drop_list = ['RemnantType_0',  'RemnantType_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2e590-6ac0-475a-aa5a-2f63194c7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT files processing\n",
    "output = pd.read_csv(paths[0],                              # read the file\n",
    "                     usecols=output_column_to_read,         # read only some cols\n",
    "                     dtype=dict(zip(output_column_to_read,  # specify the types\n",
    "                                    output_column_type))).\\ #\n",
    "            rename(columns={'Mass_0':'Mass_0_out',          # rename columns\n",
    "                            'Mass_1':'Mass_1_out'})         #\n",
    "\n",
    "# mask to select only the black holes binaries, defined by RemnantType\n",
    "idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) & (output.Semimajor.notnull())\n",
    "\n",
    "# apply the mask\n",
    "output=output[idxBHBH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17420c0-8e3f-49c1-a6dd-66bf6dd2249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVOLVED files processing\n",
    "\n",
    "#extracting the alpha parameter from the path of the file \n",
    "alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\",\n",
    "                         paths[1])[0])\n",
    "\n",
    "#read the columns we are interested in from the evolved file\n",
    "evolved = pd.read_table(paths[1],                               # read file\n",
    "                        sep='\\s+',                              # separate by spaces\n",
    "                        usecols=evolved_column_to_read,         # read only some columns\n",
    "                        dtype=dict(zip(evolved_column_to_read,  # specify the types\n",
    "                                       evolved_column_type)))   #\n",
    "#NB: sep='\\s+' is needed because there are different number of spaces separareting the columns\n",
    "\n",
    "#adding the column with the alpha parameter\n",
    "evolved['alpha'] = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb38b5f-412f-49f1-a4a5-83371b9131b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGFILE files processing\n",
    "\n",
    "logfile = pd.read_csv(paths[2],    # read the file\n",
    "                      header=None) # there is no header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c3a94-4baf-42a4-8ade-a82a7a0e40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Regex on the line of the logfile to extrac useful informations\n",
    "df_RLO = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\  # searching for string \"RLO_BEGIN\"\n",
    "            dropna().\\                                                    # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID'}).\\                          # rename columns\n",
    "            groupby(['name']).\\                                           # grouping by name\n",
    "            size().\\                                                      # and counting the number of RLO\n",
    "            to_frame(name='RLO').\\                                        # \n",
    "            reset_index()                                                 # to have a nice dataframe\n",
    "\n",
    "\n",
    "df_CE = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\  # searching for string \"CE\"\n",
    "            dropna().\\                                            # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID'}).\\                  # rename\n",
    "            groupby(['name']).\\                                   # grouping by name\n",
    "            size().\\                                              # \n",
    "            to_frame(name='CE').\\                                 # and counting the number of CE\n",
    "            reset_index()                                         # to have a nice dataframe\n",
    "\n",
    "\n",
    "df_BSN = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\  #searching for string \"BSN\"\n",
    "            dropna().\\                                              # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID'}).\\                    #rename\n",
    "            groupby(['name']).\\                                     #grouping by name\n",
    "            size().\\                                                #\n",
    "            to_frame(name='BSN').\\                                  #and counting the number of BSN\n",
    "            reset_index()                                           #to have a nice dataframe\n",
    "\n",
    "df_No_Kick = logfile[0].str.extract(r\"S;((?:\\d*\\_)?\\d+);(\\d+);SN;.+:(0):.+:.+:.+.\").\\ #searching for string \"No_Kick\"\n",
    "            dropna().\\                                                                # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID', 2: 'No_Kick'}).\\                        #rename\n",
    "            groupby(['name']).\\                                                       #grouping by name\n",
    "            size().\\                                                                  #\n",
    "            to_frame(name='No_Kick').\\                                                #and counting the number of \"No_Kick\"\n",
    "            reset_index()                                                             #to have a nice dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551766e-ef24-4461-8939-eb5337638a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGE\n",
    "bhbh = evolved.merge(output, on=['name'], how='inner').\\    #innerg join on the name between evolved and output\n",
    "               merge(df_RLO, on=['name'], how='left').\\     #left join on the name with df_RLO\n",
    "               merge(df_CE,  on=['name'], how='left').\\     #left join on the name with df_CE\n",
    "               merge(df_BSN, on=['name'], how='left').\\     #left join on the name with df_BSN\n",
    "               merge(df_No_Kick, on=['name'], how='left').\\ #final join on the name with No_Kicks\n",
    "               fillna(value=0).\\                            #setting nan to zero\n",
    "               drop(columns=drop_list)                      #dropping no longer useful columms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fea7d5-3ffa-47bd-a6b6-24190b4bff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding some columns with physical meaning\n",
    "bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime'] #time delay\n",
    "\n",
    "#defining the max mass of output\n",
    "bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                        where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']),\n",
    "                              other=bhbh['Mass_0_out'])\n",
    "\n",
    "#defining q=m1/m2 with m2>,m1\n",
    "bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "bhbh['q'] = bhbh['q'].\\\n",
    "            where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']),\n",
    "                  other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "\n",
    "#defining the Chirp mass\n",
    "bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89983d2-e16f-4d15-b8f7-6cd1db1a8e14",
   "metadata": {},
   "source": [
    "We apply the above function with Dask to the whole dataset via the Bag objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4097c15-07bc-46af-92f7-1cf0f579325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag of lists generated by a list comprehension\n",
    "bag=db.from_sequence([\n",
    "            [dir_ + f'/0/output_{thread}.csv', \n",
    "             dir_ + f'/0/evolved_{thread}.dat',\n",
    "             dir_ + f'/0/logfile_{thread}.dat'] for dir_ in dir_list for thread in range(30)],\n",
    "             npartitions=30*60) #the number of partitions is set as such in order to have more or less one list per partition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc2122-9ac4-40d2-afee-23d4b6decb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_bag_of_thread(paths):\n",
    "    '''\n",
    "    The missing code from the function above \n",
    "    '''\n",
    "    return bhbh #a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de91002e-6e0b-4f9f-898e-975b553bd7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_df = bag.map(preprocessing_bag_of_thread) #Map the preprocessing function to the bag\n",
    "bag_of_dicts = bag_of_df.map(lambda df: df.to_dict(orient='records'))\\\n",
    "                        .flatten() #force a mapping of the bag to dictionaries in order to use the .to_dataframe() method\n",
    "bhbh = bag_of_dicts.to_dataframe() #delayed function \n",
    "bhbh = bhbh.compute() #actual compute that returns the final pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a51ee-39dd-4a6d-800a-051b0a513d7c",
   "metadata": {},
   "source": [
    "# A recap on Machine Learning approaches used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde1ee2c-c900-499a-b606-60ae80586107",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "The first approach we used in order to tackle the task was to build a neural network (NN) in order to predict the mass ratio and the chirp mass, so that we could infer their distributions. \n",
    "We opted for this approach due to its flexibility, since the architecture of the network can be adapted and tailored for what is needed, from changing its depth to the activation functions used for each layer, the number of nodes and the objective of the task.\n",
    "In the following sections are presented how the task was modeled and what kind of architectures we tried to use in order to predict the values for the mass ratios and the chirp masses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e701782-bce0-4c4a-9e41-b02b24e0c25d",
   "metadata": {},
   "source": [
    "## Regression Vs Classification\n",
    "Since the quantities of which we want to learn the distribution are continous, the problem can be modeled in different ways depending on how we want to deal with them:\n",
    "* Keeping its continous nature, the task can be formulated as a **regression** where the algorithm, starting from the initial features of each record, tries to predict the exact value of the final quantity as close as possible by approximating the complex dependencies from the input values. This task can be become extremely computationally heavy as the complexity of the data increases. This is due to the size of the network needed to capture such intricate relatioships;\n",
    "* A different approach in dealing with continous target values is to discretize them by assigning different labels to different ranges of their values. This allows to characterize each data record with its own label. In this case the problem can be tackled by a classification algorithm which will try to predict the label instead of the exact value of the discretized quantity. As the number of labels increases, the complexity of the task grows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6d2720-a45a-460d-908a-48eba8015f42",
   "metadata": {},
   "source": [
    "## Direct Prediction Vs SoftMax Approach\n",
    "\n",
    "In trying to predict the correct label in a classification algorithm its `objective` can be modified in order to gain more insights on what the model is learning. By objective is intended the way the output of the task is presented.\n",
    "In this case we will refer to a direct prediction when the output consists in just the label infered from the initial input values, while an alternative approach can be returing the probability associated with each label to be assigned to a praticular record.\n",
    "This is achived by using as the output activation function the `Softmax` function:\n",
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 250px; max-height: 50px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>$$\n",
    "\\sigma(z)_j  = \\frac{e^{z_j}}{\\sum_{k} e^{z_k}}\n",
    "$$</div></center>\n",
    "Here for each output node value $z_j$, the number of which is equal to the number of possible labels, is associated the probability $\\sigma(z)_j $ of being guessed for the particular record used as input of the NN.\n",
    "By inspecting the resulting probability distributions several quantities can be infered such as the most probable label as well the dispersion of the distribution which can give insights on how well the model is learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef48556-5235-41fd-afb9-d0bc1c58558a",
   "metadata": {},
   "source": [
    "## Tested Architectures\n",
    "For each of the following proposed architecture several variants have been tested in a grid search fashion, in order to improve the capabilities of the algorithms.\n",
    "Due to the extreme complexity of the data neither regression-like tasks, nor the classification ones yielded satisfying results. Every NN has been implemented by the `Pytorch` python package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c25a8-af51-418b-b595-976d0fc4a4d9",
   "metadata": {},
   "source": [
    "### Regression Architecture\n",
    "The network is designed with fully connected layers that converge into a single output node.\n",
    "In a convolutional network fashion, the algorithm present parallel layers disconnected one from another so that each distinct group can focus on learning different aspects of the input data, for later being recombined into a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a002b5-7642-4d18-8ed3-23c15fd460e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.in1 = nn.Linear(6,4)\n",
    "        self.in2 = nn.Linear(6,4)\n",
    "        self.in3 = nn.Linear(6,4)\n",
    "        # self.in4 = nn.Linear(6,4)\n",
    "\n",
    "        self.lin1_1 = nn.Linear(4,4)\n",
    "        self.lin1_2 = nn.Linear(4,4)\n",
    "        self.lin1_3 = nn.Linear(4,4)\n",
    "        \n",
    "        self.out = nn.Linear(12,1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        x1 = self.in1(x)\n",
    "        x1 = torch.relu(x1)\n",
    "        \n",
    "        x1 = self.lin1_1(x1)\n",
    "        x1 = torch.relu(x1)\n",
    "        \n",
    "        x2 = self.in2(x)\n",
    "        x2 = torch.relu(x2)\n",
    "        \n",
    "        x2 = self.lin1_2(x2)\n",
    "        x2 = torch.relu(x2)\n",
    "        \n",
    "        x3 = self.in3(x)\n",
    "        x3 = torch.relu(x3)\n",
    "        \n",
    "        x3 = self.lin1_3(x3)\n",
    "        x3 = torch.relu(x3)\n",
    "        \n",
    "        \n",
    "        output = torch.concat((x1, x2, x3), axis=1)\n",
    "        output = self.out(output)\n",
    "        output = torch.relu(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a67e28-7f16-4ee5-91b5-c0846c8e69a4",
   "metadata": {},
   "source": [
    "<center style=\"margin-left: 10%; margin-right: 10%; background-color: #eeeeee; padding-top: 10px; padding-bottom: 10px;\"><figure><img src=\"Figures/lcp/NN.png\" width=\"60%\" height=\"30%\"><figcaption></figcaption></figure></center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef4a255-1ff5-42ed-b539-951f4dbc5f65",
   "metadata": {},
   "source": [
    "<center style=\"margin-left: 10%; margin-right: 10%; background-color: #eeeeee; padding-top: 10px; padding-bottom: 10px;\"><figure><img src=\"Figures/lcp/regression_distribution.png\" width=\"100%\" height=\"30%\"><figcaption></figcaption></figure></center><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cedc65-a814-45c4-bb05-63e5a701b246",
   "metadata": {},
   "source": [
    "### Softmax Architecture\n",
    "The NN is made of fully connected layers of increasing size, which correspond to the number of nodes they are made of.\n",
    "In the final layer, for each output node is computed the softmax function which yields the probability distribution of predicting the label for a given input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abea00c8-8508-4e53-8342-2b8c7f7c71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.input = nn.Linear(5, 10)\n",
    "        self.linear1 = nn.Linear(10, 15)\n",
    "        self.output = nn.Linear(15, 20)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        x = self.input(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        \n",
    "        x = self.output(x)\n",
    "        x = F.softmax(x, dim=0)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568edf90-bab6-4579-a793-84b62abf2fe2",
   "metadata": {},
   "source": [
    "<center style=\"margin-left: 10%; margin-right: 10%; background-color: #eeeeee; padding-top: 10px; padding-bottom: 10px;\"><figure><img src=\"Figures/lcp/NNsm.png\" width=\"100%\" height=\"30%\"><figcaption></figcaption></figure></center><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f558cdc-8445-46e0-8e2e-fba659108c1a",
   "metadata": {},
   "source": [
    "<center style=\"margin-left: 10%; margin-right: 10%; background-color: #eeeeee; padding-top: 10px; padding-bottom: 10px;\"><figure><img src=\"Figures/lcp/sm_distribution.png\" width=\"100%\" height=\"30%\"><figcaption></figcaption></figure></center><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19f270-0b45-4975-9c48-4f567125ea62",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "\n",
    "Our final choice of algorithm to implement is the `XGBoost` classifier. \n",
    "XGBoost is a python library that is optimized for distributed gradient boosting designed to be efficient and flexible.\n",
    "In particular, XGBoost is used in Supervised Learning frameworks for different tasks: for example classification or regression.\n",
    "It provides a parallell tree boosting that can be used to tackle our problem of predicting the mass distribution given the initial conditions of a binary system.\n",
    "\n",
    "The goal of our analysis is to develop an algorithm that takes as input some features of the binary system (namely the intial masses of the stars, the metallicity, the $\\alpha$ parameter, the eccentricity and the semimajor-axis) and returns the probability distribution of a chosen feature (for example $q$ or the $\\mathcal{M}$).\n",
    "Such algorithm can be obtained specifying as `objective` the `multi:softprob` as parameter of the XGBoost.\n",
    "As anticipated in the previous section, we discretized the $q$ or the $\\mathcal{M}$ so as to the resulting training is aimed at a multiclass calssification (`multi`) that returns for each record the probability of being in each class (`softprob`).\n",
    "The other parameters have been specified based on the results of a grid search.\n",
    "\n",
    "In the following cell we report the code that defines the parameters of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f26bbb-11f9-4806-866e-964401cf16c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_depth = 0              #maximum depth of a tree\n",
    "learning_rate = 0.2        #learning rate for optimization algorithm\n",
    "grow_policy = 'lossguide'  #controls the way new nodes are added; one of the possible choices\n",
    "n_estimators = 5           #equivalente to num_boosted_rounds(); Get number of boosted rounds\n",
    "max_leaves = 400           #maximum number of nodes to be added\n",
    "\n",
    "param = {'objective': 'multi:softprob', #classificatio that returns probabilities\n",
    "         'tree_method': 'hist',         #one of the possible choices\n",
    "         'n_estimators' : n_estimators,\n",
    "         'grow_policy' : grow_policy, \n",
    "         'max_depth' : max_depth,\n",
    "         'learning_rate' : learning_rate,\n",
    "         'max_leaves' : max_leaves,\n",
    "         'num_class': n_bins,           #number of classes\n",
    "         'n_jobs' : -1,                 #for parallel processing\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7170c76-6bb5-4ce9-bf40-144768bcbaac",
   "metadata": {},
   "source": [
    "In the following cell we create a DaskDMatrix which is the data structure used for training by the dask implementation of XGBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319bc21c-0744-4bb6-8703-49eb02f5cde0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtrain = xgb.dask.DaskDMatrix(client, features, label=labels, weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c00cfc-b70c-4fdf-8990-33e783508fb4",
   "metadata": {},
   "source": [
    "The `weights` parameter can be added to deal with unbalanced datasets, e.g. a dataset with different occupancies for each class.\n",
    "The way we defined the weights is as follows and we display the assigned to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc98f2e-8fee-49f0-b608-3c502726d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "countings = bhbh_train['label'].to_frame().groupby('label').size().compute()\n",
    "total_counts = countings.sum()\n",
    "\n",
    "#Assign to a new column\n",
    "def assign_weight(i):\n",
    "    return np.log(total_counts/(n_bins*countings[i])+1) \n",
    "\n",
    "bhbh_train['weight'] = bhbh_train['label'].map(assign_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d63b62-81da-45e5-aac2-5201b5522527",
   "metadata": {},
   "source": [
    "# add image of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db5a685-4882-4d2a-a262-a30c81214e97",
   "metadata": {},
   "source": [
    "Then we proceed with the training of the XGBoost with the chosen parameters and the given training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dc22d-2508-499c-a3f0-727e4953cac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train XGB\n",
    "bst = xgb.dask.train(client, param, dtrain, evals=[(dtrain, \"train\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb70ff9-7605-4ac4-9239-da6c1a0e35ef",
   "metadata": {},
   "source": [
    "When the model is trained, we apply it to the test dataset to obtain some predictions on new data.\n",
    "In particular the algorithm will return the probability distribution of each record of belonging to each class. \n",
    "To predict a particular label we can then extract the class that has the highest probability and confront it with the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8d859-82ba-4712-86ae-1c7ac40634ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## distributions of the probability of each class for each record\n",
    "output_test = xgb.dask.predict(client, bst, features_test).persist() \n",
    "\n",
    "## Predicition of the label with Maximum estimation\n",
    "predicted_label = output_test.idxmax(axis=1).persist()\n",
    "\n",
    "## Computing the true labels and the predicted\n",
    "\n",
    "#true labels\n",
    "Y_test = label_test.values.compute()\n",
    "#predicted labels\n",
    "Y_predicted = predicted_label.values.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7562655-440e-41b4-b103-8cb4fe9a13ba",
   "metadata": {},
   "source": [
    "The results will be presented in the next sections of this report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b96dab6-c83c-4d51-a98d-affe9c537e94",
   "metadata": {},
   "source": [
    "## Grid search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7cab9f-281b-4d10-8de6-d40cf0caebc5",
   "metadata": {},
   "source": [
    "In order to determine the best hyperparameters for the model we implement a grid search on different values of the `max_depth`, the `learning_rate`, the `n_estimators` and `max_leaves`.\n",
    "\n",
    "The grid search has been done by creating a list of possible values for each parameter and cyclying with for loops over them, testing each combination. \n",
    "\n",
    "In order to compare different results we have introduced some metrics, like the Kullback-Leibler, the Jensen-Shannon divergence and an accuracy measure defined by us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363a112-4656-42d5-a17e-dd75e3f0fc40",
   "metadata": {},
   "source": [
    "#### Kullback-Leibler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1817c69-bf48-4401-b98f-f42d33870002",
   "metadata": {},
   "source": [
    "The Kullback-Leibler divergence (also called relative entropy) is a type of statistical distance that measures the distance between two distributions $p$ and $q$. \n",
    "It is defined as:\n",
    "$$ D_{KL}(p||q)=\\sum_i p_i \\cdot \\log\\left(\\frac{p_i}{q_i}\\right) $$\n",
    "\n",
    "In particular, $D_{KL}=0$ if the two distributions are identical. It is already implemented in the `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef78656b-478f-45ae-976e-61d971ca70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "KL_div = scipy.stats.entropy(counts_predicted, counts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddff48e-24d0-48a2-aab9-e9e416253cdb",
   "metadata": {},
   "source": [
    "#### Jensen-Shannon divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acc80c-a44a-4b9b-9e97-aec58427c903",
   "metadata": {},
   "source": [
    "The Jensen-Shannon divergence is another type of statistical distance, based on the Kulback-Leibler with the difference that it is symmetric and has a finite value (while the KL divergence does not have an upper limit).\n",
    "\n",
    "It is defined as:\n",
    "\n",
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 300px; max-height: 50px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>\n",
    "$$ D_{JS}(p||q)=   \\frac{1}{2}D_{KL}(p||m)+ \\frac{1}{2} D_{KL}(q||m)$$\n",
    "    </div></center>\n",
    "\n",
    "where $m={\\frac {1}{2}}(p+q)$. It is also already implemented in the `scipy` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fe171-545c-419b-b5c8-ce700b093d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jensenshannon as JS\n",
    "JS_div = JS(counts_test, counts_predicted, base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a77a6-2028-4f7a-b018-439f7bd7d7dc",
   "metadata": {},
   "source": [
    "In the grid search, the above defined measures have been used to compare the true distributions of the $q$ and the $\\mathcal{M}$ of the test set and the one predicted by our algorithm.\n",
    "It is difficult to interpret these measures by themeselves because we do not know if a given distance is a good result or not. We can anyway use these measures to compare different models in the grid search by selecting the one that minimizes the distances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54412ab8-a8fb-4e85-96d3-e15c64dcc8a6",
   "metadata": {},
   "source": [
    "#### A threshold accuracy measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d174bc-5b73-4de1-9cfa-2640659e0939",
   "metadata": {},
   "source": [
    "The typical measure of accuracy of classification task consists in calculating the percentange of correctly predicted labels is not very informative in our problem.\n",
    "We are actually dealing with a pretty large number of classes ($100$) and therefore we would like to consider the fact that if we predict a label that is near the true one we are not completely unsatisfied.\n",
    "For this reason we define an accuracy that counts the percentange of corrected labels within a tollerance defined by a threshold. \n",
    "This accuracy is useful to understand what is the percentange of labels that we correctly predict within a given tollerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61a179-911b-4c30-9f7a-eeb7d192e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining a prediction accurate if not further from the true of a value of threshold\n",
    "def accuracy_threshold(Y_test, Y_predicted, threshold = 5):\n",
    "    out=np.abs(Y_test-Y_predicted)\n",
    "    mask = out < threshold\n",
    "    return np.sum(mask)/len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67d92a0-a6c9-484e-b1c8-e2620fe3e92c",
   "metadata": {},
   "source": [
    "We can now finaly present the code that we have written in order to run the grid search, based on nested for loops that cycles on the list of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86ed5d-ac9a-431e-aaa9-f4d8428efa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters lists\n",
    "max_depth_list = [4, 6, 8, 10, 0]\n",
    "learning_rate_list = [0.1, 0.2, 0.3]\n",
    "grow_policy = 'lossguide'\n",
    "n_estimators_list = [5, 15, 20]\n",
    "max_leave_list = [50, 100, 200, 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa01dd-5e79-4336-a064-d184a109175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRID SEARCH\n",
    "\n",
    "#file in wich to save the results of the grid search\n",
    "f = open('grid_search_results.csv', 'a')\n",
    "writer = csv.writer(f)\n",
    "\n",
    "#writing the head of the file\n",
    "head = ['max_depth', 'learning_rate', 'n_estimators', 'max_leaves', 'KL_div', 'JS_div', 'AT']\n",
    "writer.writerow(head)\n",
    "\n",
    "#and printing to std output\n",
    "print('max_depth', 'learning_rate', 'n_estimators', 'max_leaves', 'KL_div', 'JS_div', 'AT')\n",
    "\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    \n",
    "    for learning_rate in learning_rate_list:\n",
    "        \n",
    "        for n_estimators in n_estimators_list:\n",
    "            \n",
    "            for max_leaves in max_leave_list:\n",
    "                \n",
    "                #selected parameters\n",
    "                param = {'objective': 'multi:softprob',\n",
    "                         'tree_method': 'hist', \n",
    "                         'n_estimators' : n_estimators, #equivalent to num_boost_round, lets keep it in params\n",
    "                         'grow_policy' : grow_policy, \n",
    "                         'max_depth' : max_depth,\n",
    "                         'learning_rate' : learning_rate,\n",
    "                         'max_leaves' : max_leaves,\n",
    "                         'num_class': n_bins,\n",
    "                         'n_jobs' : -1,\n",
    "                        }\n",
    "                \n",
    "                #training the model\n",
    "                bst = xgb.dask.train(client, param, dtrain, evals=[(dtrain, \"train\")])\n",
    "                \n",
    "                #extracting the predictions\n",
    "                output_test = xgb.dask.predict(client, bst, features_test).persist() #already a dataframe\n",
    "                predicted_label = output_test.idxmax(axis=1).persist()\n",
    "                Y_test = label_test.values.compute()\n",
    "                Y_predicted = predicted_label.values.compute()\n",
    "                \n",
    "                #Kulback-Leibler\n",
    "                counts_test, _ = np.histogram(Y_test, density=True, bins=n_bins)\n",
    "                counts_predicted, _ = np.histogram(Y_predicted, density=True, bins=n_bins)\n",
    "                \n",
    "                KL_div = scipy.stats.entropy(counts_predicted, counts_test)\n",
    "                \n",
    "                ## Jensen-Shannon divergence\n",
    "                JS_div = JS(counts_test, counts_predicted, base=2)\n",
    "                \n",
    "                #Accuracy threshold\n",
    "                AT = accuracy_threshold(Y_test, Y_predicted, 15)\n",
    "                \n",
    "                #saving the results in a csv file\n",
    "                row = [max_depth, learning_rate, n_estimators, max_leaves, KL_div, JS_div, AT] \n",
    "                writer.writerow(row)\n",
    "                \n",
    "                #printing to std output\n",
    "                print(max_depth, learning_rate, n_estimators, max_leaves, KL_div, JS_div, AT)\n",
    "                \n",
    "                \n",
    "\n",
    "#closing the file\n",
    "f.close()              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037c5afe-b2df-4c94-bca2-f2d8694d739d",
   "metadata": {},
   "source": [
    "The results have been finally saved in a `csv` file. \n",
    "With all the three tested learning rates we obtain the same results, meaning the all the three values reach convergence. \n",
    "We have then chosen as best parameter $learning\\_rate =0.2$. Also the number of estimators does not affect the results, we have then kept the lowest one, $n\\_estimator = 5$. \n",
    "For the maximum leaves we obtain as best parameter $max\\_leaves= 400$ and as expected, the perfomances improves by increasing the maximum depth and the best parameter is $max\\_depth=0$ (which means that the algoritms stops only when it reaches convergece).\n",
    "\n",
    "In the following table we report the best parameters. These parameters are the ones that we use for the rest of the analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10821c57-0830-407b-acd2-f9f5b2d55d8f",
   "metadata": {},
   "source": [
    "Parameter | Value\n",
    "---|:---: \n",
    "max_depth| 0\n",
    "learning rate|0.2\n",
    "number of estimators| 5\n",
    "maximum leaves | 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00f570-0b98-4881-a591-48e3f9fcedab",
   "metadata": {},
   "source": [
    "# Discussion on the predictions\n",
    "The algorithm is able to perform predictions either on the `q` or the `chirp mass`, as its implementation barely varies.\n",
    "\n",
    "Here we are showing the results of the predictions made with `q`. In the following section we are showing the same results but run with the `chirp mass` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71095a27-3921-4f04-b0ce-6ad53d84338b",
   "metadata": {},
   "source": [
    "### Feature importance\n",
    "A first analysis of the performance of the model is carried out through the feature importance.\n",
    "This analysis shows how much each of the features of the training dataset is used to split the data in the XGBoost architecure.\n",
    "Other similar importance metrics such as the gain, which is the average of the gain per split when a specific feature was used, show that `Mass_0`, `Z_0` the metallicity and `Mass_1` were the most useful and could better help the training process.\n",
    "We remind the feature importance does not take into account the test set, as it is just another way of displaying the information learnt dirung the training.\n",
    "\n",
    "<!--center style=\"margin-left: 5%; margin-right: 5%; background-color: #eeeeee; padding-top: 10px; padding-bottom: 10px;\"-->\n",
    "<center>\n",
    "<table><tbody><tr>\n",
    "<td> <img src=\"Figures/lcp/importance_q_yeskicks_weight.png\"></td>\n",
    "<td> <img src=\"Figures/lcp/importance_q_yeskicks_gain.png\" ></td>\n",
    "<td><img src=\"Figures/lcp/importance_q_yeskicks_total_gain.png\"> </td>\n",
    "</tr></tbody>\n",
    "    <tfoot>Importance fo each feature with different scorings</tfoot>\n",
    "</table>\n",
    "</center><br>\n",
    "\n",
    "Our following analysis distinguishes the systems that do not display any kicks in the binary evolution.\n",
    "To get these results we run the whole training and prediction on a subset of systems, namely only those characterized by the flag $NoKicks=2$. \n",
    "We make this distinction because we are trying to understand how the random component introduced by the kicks alters the accuracy of the predictions.\n",
    "However for the previous graphs the results coming out of these two approaches show negligible differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b79f5-fb4d-4e2e-8eb3-8bd0123e62d0",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "To check by eye how the algorithm performs on the unseen test set, we plot the confusion matrix of the predictions.\n",
    "\n",
    "<center>\n",
    "<table><tbody><tr>\n",
    "<td><img src=\"Figures/lcp/confusion_matrix_labels_yeskicks.png\"></td>\n",
    "<td> <img src=\"Figures/lcp/confusion_matrix_labels_nokicks.png\" ></td>\n",
    "</tr></tbody>\n",
    "    <tfoot><td>All systems</td><td>Nokick</td></tfoot>\n",
    "</table>\n",
    "</center><br>\n",
    "\n",
    "Each square of the picture shows the number of counts that were assigned to a specific label, given the value of the true label.\n",
    "It is immediate to spot higher countings of binary systems that are in accordance with their true label, thus suggesting that the model really understood the hidden processes.\n",
    "\n",
    "Finally it is easy to spot how higher labels tend to be more frequently misclassified, but this is easily explained by the unbalanced dataset we started with.\n",
    "Very few binary systems of actual lower q labels happen to be missclassified with high labels as shown by the white patch in the upper left corner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df43fcd-861b-464d-98ed-cd615a6dc99e",
   "metadata": {},
   "source": [
    "### Comparison of the empirical and predicted `q` label distributions\n",
    "A different and more comprehensive visualization to better check the performances of the model is to plot the distributions of the empirical and true labels, as shown in the following plots.\n",
    "\n",
    "<center>\n",
    "<table><tbody><tr>\n",
    "<td><img src=\"Figures/lcp/hist_q_yes_kicks.png\"></td>\n",
    "<td> <img src=\"Figures/lcp/hist_q_nokicks.png\" ></td>\n",
    "</tr></tbody>\n",
    "    <tfoot><td>All systems</td><td>Nokick</td></tfoot>\n",
    "</table>\n",
    "</center><br>\n",
    "\n",
    "\n",
    "The distributions seem to match very well, in all the various regions of the q labels.\n",
    "For the whole dataset, the model seems to capture, but only up to a certain extent, the bump around the region of labels 60 to 70 as there are some deviations from the smooth trend.\n",
    "There seems to be a slight overcouting for the smaller labels compared to the true distribution, but as can be seen from the Kullback-Liebler and Jensen-Shannon metrics, this does not influence the overall correspondance between the two distributions.\n",
    "However, for the systems without the kicks, the bump is absent and so is for the predicted distribution.\n",
    "As can be seen from the comparison of the metrics, the second predicted distribution is more closely matching to the empirical data.\n",
    "\n",
    "Additionally we plot the so called $\\Delta q$ which measures how much and how often labels were overestimated or underestimated.\n",
    "We obtain a 95% confidence interval of $\\Delta q$ ranging from -43 to 24 for the whole dataset and from -31 to 15 for the nokicks subset. Furthermore, this observation suggests that our algorithm is sensible to the stochasticity of the binary evolution.\n",
    "\n",
    "To better display the way labels are missassigned we plot in the inset the logarithmic distributions which shows an asymmetry to the left: labels are more prone to be underestimated.\n",
    "\n",
    "<center>\n",
    "<table><tbody><tr>\n",
    "<td><img src=\"Figures/lcp/hist_delta_q_yeskicks.png\"></td>\n",
    "<td> <img src=\"Figures/lcp/hist_delta_q_nokicks.png\" ></td>\n",
    "</tr></tbody>\n",
    "    <tfoot><td>All systems</td><td>Nokick</td></tfoot>\n",
    "</table>\n",
    "</center><br>\n",
    "\n",
    "More specifically, within an error of $\\Delta q=25$ in the assignement of the labels, we have an accuracy of 90% for the whole dataset, whereas the same accuracy for the nokicks dataset is achieved with a $\\Delta q=13$.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb4c85-9938-44d7-88e8-8cdec4de0940",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "787a999c-901c-42e1-9bea-85a60c379e7e",
   "metadata": {},
   "source": [
    "## Analysis with the `chirp mass`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9267ee7-6b36-43e3-b540-b8815075c9ae",
   "metadata": {},
   "source": [
    "# Final consideration and conclusions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
