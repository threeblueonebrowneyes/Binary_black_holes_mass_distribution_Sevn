{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14dde600-a738-4e9b-91d9-0edae540f483",
   "metadata": {},
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>\n",
    "    <center>\n",
    "        <div style='  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;'>LCP-B - Processing of SEVN data for binary black holes mass distribution analysis\n",
    "        </div>\n",
    "    <center>\n",
    "        <br>\n",
    "    <i>Tommaso Bertola, Giacomo Di Prima, Giuseppe Viterbo, Marco Zenari</i></center>\n",
    "    <center>\n",
    "    <i>All authors contributed equally to the project</i></center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abe26b-8545-4bae-8bd7-a799da7478ba",
   "metadata": {},
   "source": [
    "# The physical problem\n",
    "The subject of our studies are binary black holes obtained by the population synthesis code SEVN, in particular we aim to understand the importance of the input features of the program in guiding the evolution, and gaining some insight about the final mass distribution, and some correlated quantities.\n",
    "\n",
    "1. Chirp mass and q\n",
    "Describe\n",
    "\n",
    "\n",
    "But firstly it is important to understand what exatly are black holes, and how stars evolve into them. \n",
    "When a star ends its nuclear burning material, no sorce of energy can support the gravity pressure and the star goes out of the hydrostatic equilibrium (described as the balance between Pressior and Nuclear forces and Gravity) and the core collapse begins. In particular, if the Mass of the star is bigger than 20 M_sun the gravity is so strong that no source of pressure can stop the core collpase, and a black hole is form. \n",
    "As we know from observations, almost all the massive stars (M > 20 Msun) are in binaries (or higher multiplicity systems), so it is important to understand the evolution of binary system.\n",
    "\n",
    "\n",
    "2. How do supernova deviate our predictions\n",
    "The core collapse is one fondamental ingredient to define to the final distribution of the mass of a black hole. During the final stages of evolution the Silicon burning shell ads material to the Iron core, and when this one gets to a total mass bigger than the Chandrasekar Mass (1.3-1.7 M_sun), the degenerate electron pressure is not able to sostain anymore the gravity, and so the runaway core collapse begins. -insert equation- \n",
    "When the shock wave is not stopped the star explode as a SN, leaving a compact remant, either a NS or a low-mass BH (is it true? we should check if the Direct Collapse produce more massive black holes maybe). When such an event occur, asymmetry in mass ejection or in the neutrino emission (cool down the shockwave generally), or also symmetric mass loss in binary system, leads to the so called Kicks, which is the gained momentum obtained by the compact object after the SN. Unfortunalty for black holes supernova kicks there is no real observation (model selected by seven), and the kick is extracte from a model dependent proability distribution -our simulation where from Freyer 2012, need to check-. In our algorithm we try to derive how much this kind of phenomena is able to deviate our prediction of the output mass distribution.\n",
    "\n",
    "\n",
    "3. How do the evolution in a binary system interfere with the evolution?\n",
    "For the best predicted records we also investigate the history evolution, focusing on the mass exchange channels RLO and CE. \n",
    "\n",
    "\n",
    "\n",
    "For this reason we examin the following parameter space:\n",
    "\n",
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| [M_0, M_1]  | Initial masses of the stars in the binary   |\n",
    "| Z         | Metallicity of the two starts in the binary |\n",
    "| a         | semi-major axis of the binary               |\n",
    "| e         | eccentricity of the binary                  |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Our aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d43794c-2302-4f75-9d51-ddbad97bfc13",
   "metadata": {},
   "source": [
    "# A brief recap on the preprocessing of the data\n",
    "Given the initial 1.7TB of data, we need to first preprocess it in order to extract only the meaningful information.\n",
    "To achieve this goal we use a cluster of 6 VMs hosted in CloudVeneto leveraging the capabilities of Python Dask library.\n",
    "\n",
    "We show below the implementation of the code used to parse the whole dataset. The data extracted from this algorithm is then saved onto disk to avoid rerunning the whole computation which takes about 2 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb9f44-3b16-4f09-adee-686ccfb7249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_{}.csv\n",
    "output_column_to_read = ['name', 'Mass_0', 'RemnantType_0',\n",
    "                         'Mass_1', 'RemnantType_1',\n",
    "                         'Semimajor','Eccentricity',\n",
    "                         'GWtime','BWorldtime']\n",
    "output_column_type = ['string', 'float64', 'int64',\n",
    "                      'float64', 'int64',\n",
    "                      'float64', 'float64',\n",
    "                      'float64', 'float64']\n",
    "\n",
    "# evolved_{}.dat\n",
    "evolved_column_to_read = ['name', 'Mass_0',\n",
    "                          'Z_0', 'SN_0',\n",
    "                          'Mass_1', 'SN_1',\n",
    "                          'a', 'e']\n",
    "evolved_column_type = ['string', 'float64',\n",
    "                       'float64', 'string',\n",
    "                       'float64', 'string',\n",
    "                       'float64', 'float64']\n",
    "\n",
    "# further columns to remove at the end \n",
    "drop_list = ['RemnantType_0',  'RemnantType_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca2e590-6ac0-475a-aa5a-2f63194c7d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OUTPUT files processing\n",
    "output = pd.read_csv(paths[0],                              # read the file\n",
    "                     usecols=output_column_to_read,         # read only some cols\n",
    "                     dtype=dict(zip(output_column_to_read,  # specify the types\n",
    "                                    output_column_type))).\\ #\n",
    "            rename(columns={'Mass_0':'Mass_0_out',          # rename columns\n",
    "                            'Mass_1':'Mass_1_out'})         #\n",
    "\n",
    "# mask to select only the black holes binaries, defined by RemnantType\n",
    "idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) & (output.Semimajor.notnull())\n",
    "\n",
    "# apply the mask\n",
    "output=output[idxBHBH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17420c0-8e3f-49c1-a6dd-66bf6dd2249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVOLVED files processing\n",
    "\n",
    "#extracting the alpha parameter from the path of the file \n",
    "alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\",\n",
    "                         paths[1])[0])\n",
    "\n",
    "#read the columns we are interested in from the evolved file\n",
    "evolved = pd.read_table(paths[1],                               # read file\n",
    "                        sep='\\s+',                              # separate by spaces\n",
    "                        usecols=evolved_column_to_read,         # read only some columns\n",
    "                        dtype=dict(zip(evolved_column_to_read,  # specify the types\n",
    "                                       evolved_column_type)))   #\n",
    "#NB: sep='\\s+' is needed because there are different number of spaces separareting the columns\n",
    "\n",
    "#adding the column with the alpha parameter\n",
    "evolved['alpha'] = alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb38b5f-412f-49f1-a4a5-83371b9131b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGFILE files processing\n",
    "\n",
    "logfile = pd.read_csv(paths[2],    # read the file\n",
    "                      header=None) # there is no header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c3a94-4baf-42a4-8ade-a82a7a0e40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running Regex on the line of the logfile to extrac useful informations\n",
    "df_RLO = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\  # searching for string \"RLO_BEGIN\"\n",
    "            dropna().\\                                                    # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID'}).\\                          # rename columns\n",
    "            groupby(['name']).\\                                           # grouping by name\n",
    "            size().\\                                                      # and counting the number of RLO\n",
    "            to_frame(name='RLO').\\                                        # \n",
    "            reset_index()                                                 # to have a nice dataframe\n",
    "\n",
    "\n",
    "df_CE = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\  # searching for string \"CE\"\n",
    "            dropna().\\                                            # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID'}).\\                  # rename\n",
    "            groupby(['name']).\\                                   # grouping by name\n",
    "            size().\\                                              # \n",
    "            to_frame(name='CE').\\                                 # and counting the number of CE\n",
    "            reset_index()                                         # to have a nice dataframe\n",
    "\n",
    "\n",
    "df_BSN = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\  #searching for string \"BSN\"\n",
    "            dropna().\\                                              # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID'}).\\                    #rename\n",
    "            groupby(['name']).\\                                     #grouping by name\n",
    "            size().\\                                                #\n",
    "            to_frame(name='BSN').\\                                  #and counting the number of BSN\n",
    "            reset_index()                                           #to have a nice dataframe\n",
    "\n",
    "df_No_Kick = logfile[0].str.extract(r\"S;((?:\\d*\\_)?\\d+);(\\d+);SN;.+:(0):.+:.+:.+.\").\\ #searching for string \"No_Kick\"\n",
    "            dropna().\\                                                                # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID', 2: 'No_Kick'}).\\                        #rename\n",
    "            groupby(['name']).\\                                                       #grouping by name\n",
    "            size().\\                                                                  #\n",
    "            to_frame(name='No_Kick').\\                                                #and counting the number of \"No_Kick\"\n",
    "            reset_index()                                                             #to have a nice dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551766e-ef24-4461-8939-eb5337638a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGE\n",
    "bhbh = evolved.merge(output, on=['name'], how='inner').\\    #innerg join on the name between evolved and output\n",
    "               merge(df_RLO, on=['name'], how='left').\\     #left join on the name with df_RLO\n",
    "               merge(df_CE,  on=['name'], how='left').\\     #left join on the name with df_CE\n",
    "               merge(df_BSN, on=['name'], how='left').\\     #left join on the name with df_BSN\n",
    "               merge(df_No_Kick, on=['name'], how='left').\\ #final join on the name with No_Kicks\n",
    "               fillna(value=0).\\                            #setting nan to zero\n",
    "               drop(columns=drop_list)                      #dropping no longer useful columms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fea7d5-3ffa-47bd-a6b6-24190b4bff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding some columns with physical meaning\n",
    "bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime'] #time delay\n",
    "\n",
    "#defining the max mass of output\n",
    "bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                        where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']),\n",
    "                              other=bhbh['Mass_0_out'])\n",
    "\n",
    "#defining q=m1/m2 with m2>,m1\n",
    "bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "bhbh['q'] = bhbh['q'].\\\n",
    "            where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']),\n",
    "                  other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "\n",
    "#defining the Chirp mass\n",
    "bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a51ee-39dd-4a6d-800a-051b0a513d7c",
   "metadata": {},
   "source": [
    "# A recap on Machine Learning approaches used\n",
    "## We discretize the q, ratio between masses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19f270-0b45-4975-9c48-4f567125ea62",
   "metadata": {},
   "source": [
    "## XGBoost our Lord and Saviour\n",
    "\n",
    "# Grid search\n",
    "## Accuracies used\n",
    "## What varies\n",
    "## What improves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca00f570-0b98-4881-a591-48e3f9fcedab",
   "metadata": {},
   "source": [
    "# Discussion on the predictions\n",
    "The algorithm is able to perform predictions either on the `q` or the `chirp mass`, as its implementation barely varies.\n",
    "\n",
    "Here we are showing the results of the predictions made with `q`. In the following section we are showing the same results but run with the `chrip mass` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a52aa9-ea04-4598-b95f-677130e54921",
   "metadata": {},
   "source": [
    "## Analysis with the `q`\n",
    "### Feature importance\n",
    "A first analysis of the performance of the model is carried out through the feature importance.\n",
    "This analysis shows how much each of the features of the training dataset is used to split the data in the XGBoost architecure.\n",
    "Other similar importance metrics such as the gain, which is the average of the gain per split when a specific feature was used, show that `Mass_0`, `Z_0` the metallicity and `Mass_1` were the most useful and could better help the training process.\n",
    "We remind the feature importance does not take into account the test set, as it is just another way of displaying the information learnt dirung the training.\n",
    "\n",
    "<img src='./notebooks/Figures/importance_q_yeskicks_weight.png'>\n",
    "<img src='./notebooks/Figures/importance_q_yeskicks_gain.png'>\n",
    "<img src='./notebooks/Figures/importance_q_yeskicks_total_gain.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b79f5-fb4d-4e2e-8eb3-8bd0123e62d0",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "To check by eye how the algorithm performs on the unseen test set, we plot the confusion matrix of the predictions.\n",
    "\n",
    "<img src='notebooks/Figures/lcp/confusion_matrix_labels_yeskicks.png'>\n",
    "\n",
    "Each square of the picture shows the number of counts that were assigned to a specific label, given the value of the true label.\n",
    "It is immediate to spot higher countings of binary systems that are in accordance with their true label, thus suggesting that the model really understood the hidden processes.\n",
    "Finally it is easy to spot how higher labels tend to be more frequently misclassified, but this is easily explained by the unbalanced dataset we started with.\n",
    "Very few binary systems of actual lower q labels happen to be missclassified with high labels as shown by the white patch in the upper left corner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df43fcd-861b-464d-98ed-cd615a6dc99e",
   "metadata": {},
   "source": [
    "### Comparison of the empirical and predicted `q` label distributions\n",
    "A different and more comprehensive visualization to better check the performances of the model is to plot the distributions of the empirical and true labels, as shown in the following plot.\n",
    "\n",
    "<img src='notebooks/Figures/lcp/hist_q_yes_kicks.png'>\n",
    "\n",
    "The distributions seem to match very well, in all the varying regions of the q labels.\n",
    "The model seems to capture, but only to a certain extent, the bump around the region of labels 60 to 70 as there are some deviations from the smooth trend.\n",
    "There seems to be a slight overcouting for the smaller labels compared to the true distribution, but as can be seen from the Kullback-Liebler and Jensen-Shannon metrics, this does not inflence the overall correspondance between the two distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a999c-901c-42e1-9bea-85a60c379e7e",
   "metadata": {},
   "source": [
    "## Analysis with the `chirp mass`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9267ee7-6b36-43e3-b540-b8815075c9ae",
   "metadata": {},
   "source": [
    "# Final consideration and conclusions "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
