{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ace0c39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>\n",
    "    <center>\n",
    "        <div style='  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;'>MAPD-B - Preprocessing of SEVN data for binary black holes mass distribution analysis\n",
    "        </div>\n",
    "    <center>\n",
    "        <br>\n",
    "    <i>Tommaso Bertola, Giacomo Di Prima, Giuseppe Viterbo, Marco Zenari</i></center>\n",
    "    <center>\n",
    "    <i>All authors contributed equally to the project</i></center>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5750a1fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction: the computational problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaded6b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"Figures/sevn_logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab78bf",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Our aim for this project is to preprocess the data of multiple SEVN simulations of binary systems.\n",
    "\n",
    "SEVN is a C++ program developed by the Astronomy Department at the University of Padua to simulate the evolution of binary systems. The simulation takes into account different physical phenomena regarding stellar evolution, described by stellar tracks, and also binary dynamics.\n",
    "\n",
    "We will focus our attention specifically to those systems evolving into binary black holes. To study these systems we therefore need to extract from the whole dataset produced by SEVN only some information regarding the initial and final conditions of the binary system evolution.\n",
    "\n",
    "The final goal is to obtain a simple and handy DataFrame listing only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66cea61",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Tools & techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e037bf-eb7a-432a-8d72-c897a5bbf0ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Distributed computing techniques: \n",
    "* CloudVeneto virtual machines\n",
    "* NFS file sharing\n",
    "* Python Dask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d07217",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Infrastracture configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093ad4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![network_configuration.png](network_configuration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06672cd-a3fd-421f-a387-17a69531a807",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "We will be using 6 different virtual machines, `bhbh-{d,1,2,3,4,5}`. \n",
    "The first, `bhbh-d`, serves as a NFS server, allowing all the other VMs `bhbh-{1,2,3,4,5}` to read, write and crunch the archived data as can be seen from the picture above with the green arrows.\n",
    "Highlighted in orange, all `bhbh-{1..5}` are connected to a Dask cluster, managed solely by `bhbh-1` which performs the action of client, scheduler and worker at the same time.\n",
    "\n",
    "### Setting up `bhbh-d`\n",
    "After instantiating the VM `bhbh-d` of flavor `cloudveneto.medium`, we attach the volume `bhbh-v` to `/dev/vdb` which is then mounted on `/mnt/bhbh-v`.\n",
    "To allow the other VMs to see the new volume, we use the software provided by `nfs-kernel-server`.\n",
    "\n",
    "The NFS server is configured such that `/mnt/bhbh-v` can be seen by the other VMs and therefore mounted therein.\n",
    "It is important to note that the configuration of the sharing of the volume is made in `sync` mode.\n",
    "The reason being the cluster might potentially alter the consistency of the data if `async` mode were used. \n",
    "\n",
    "Finally, the actual data were copied from the `demoblack.fisica.unipd.it` server to the mounted volume.\n",
    "The operation took a few hours as the total size of the data is around 1.7TB.\n",
    "The operation was carried out via `rsync` utility in order to resume the copying, was the network to fail for some unspecified errors.\n",
    "\n",
    "### Setting up `bhbh-{1,2,3,4,5}`\n",
    "The VMs for the proper data crunching are of the flavour `cloudveneto.large`, having 8GB of RAM each for a total amount of 40GB<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1).\n",
    "The following operations are equally repeated on all the VMs in order to have equally working systems.\n",
    "\n",
    "After installing the `nfs-utils` package, we mount the Network volume issuing the following command `mount -t nfs 10.67.22.169:/mnt/bhbh-v /mnt/bhbh`. \n",
    "For simplicty sake we use `conda` as a package manager. We install the `dask[complete]` suite of packages.\n",
    "This set up is enough to allow the cluster to work.\n",
    "\n",
    "### Particular set up of bhbh-1\n",
    "The `bhbh-1` has some slight differences in the software installed as it is used for different puropses.\n",
    "`bhbh-1` hosts the Jupyter server, acting as a client to the cluster, the cluster scheduler and also a cluster worker.\n",
    "The reason why this set up is used is just for optimizing the memory and computing resources available.\n",
    "A better configuration would have been achieved by moving the client and the scheduler to another VM but this was not easy to do as cloud resources were limited.\n",
    "Indeed, one of the main issues we came across during the tests was a lack of RAM in the `bhbh-1`, presumably due to the exchange of data between workers. For this reason we are discussing different approaches in the following.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) 40GB is comparable to the size of one folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e22c94",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab65b1fe",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "To better understad the problem, we will briefely describe the dataset we are given.\n",
    "\n",
    "The dataset consists of a number of folders named after some \"hyperparameters\" given to SEVN while performing the simulations.\n",
    "In our case, we are given 60 different folders, whose names are like `sevn_output_Z0.001A1L1`, `sevn_output_Z0.03A5L1`, ... More specifically the hyperparameters are the numbers following the letters `Z`,`A` and `L` which will be included in the final output DataFrame for each record."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac1bcb8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* 1.7TB of data\n",
    "* 60 different folders: `sevn_output_Z0.001A1L1`, ...\n",
    "\n",
    "Inside each folder there are three kinds of files:\n",
    "* `output_{nthread}.csv`, each of 750MB\n",
    "* `logfile_{nthread}.dat`, each of 200MB\n",
    "* `evolved_{nthread}.dat`, each of 50MB\n",
    "\n",
    "`{nthread}` is a number ranging from 0 to 29, corresponding to the thread responsible for the computation of those simulations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39117311",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## File schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8916aed-a4f0-4831-b92d-a0838ee01358",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We describe the schema of the three different kind of files to better explain the following parsing process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9e02f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `evolved_{nthread}.dat` schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76efb12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**fixed width** fields and therefore we used the `pd.read_table` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3612746c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These files contain the initial properties of the systems that has been successfully simulated by SEVN.\n",
    "These are **fixed width** fields files and therefore we used the `pd.read_table` function to parse them.\n",
    "The fields we are interested in reading are reported below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffcc887",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name|string `0_1234....`|**Unique in each folder**|\n",
    "|Mass_0|float|Initial mass of star 0 (in Solar masses)|\n",
    "|Mass_1|float|Initial mass of star 1 (in Solar masses)|\n",
    "|Z_0|float|Metallicity, same as in directory name (both stars have the same)|\n",
    "|SN_0|string `rapid_gauNS`|Supernova model for star 0|\n",
    "|SN_1|string `rapid_gauNS`|Supernova model for star 1|\n",
    "|a|float| Initial semimajor axis of the binary |\n",
    "|e|float|Initial eccentricity of the binary|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939d058-c06c-41d7-b34d-fd09027f27dc",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "                \n",
    "All other fields in the files are discarded in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ecd0b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `output_{nthread}.csv` schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc8daad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Simulation step for a given binary system and each column indicates a different feature of the simulation.\n",
    "The file is formatted in a tipical **csv format**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1694e62e",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Each record of the output files consists of a simulation step for a given binary system and each column indicates a different feature of the simulation.\n",
    "The file is formatted in a tipical **csv format**.\n",
    "We report in the following table the columns we are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f6db59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name |string `0_1234...`| **Unique in each folder**|\n",
    "|Mass_0|double| Mass of object 0 (in Solar masses)|\n",
    "|Mass_1|double| Mass of object 1 (in Solar masses)|\n",
    "|RemnantType_0|int|Type of object 0 after evolution|\n",
    "|RemnantType_1|int|Type of object 1 after evolution|\n",
    "|Semimajor|float|Semimajor of the binary (in Solar radius)|\n",
    "|Eccentricity|float| Eccentricity of the binary|\n",
    "|GWtime|float|Gravitational wave orbital decay time|\n",
    "|BWorldtime|int|Time elapsed in the simulations|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc3707-efcd-415d-b263-efc839157972",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All other fields in the files are discarded in the analysis.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d19e9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### `logfile_{nthread}.dat` schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ada447",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Logfiles are **plain text** files.\n",
    "\n",
    "Records are univocally associated to a unique specific binary system evolution by its `name`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1059568",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Logfiles are **plain text** files, containing further details of particular astrophysical events that happened during the simulation in each row.\n",
    "Each event is univocally associated to a specific binary system evolution by its `name`, however there could be multiple events regarding the same system.\n",
    "To recover the relevant information we have to run different regular expressions.\n",
    "Each regex used specifically captures the type of the event, namely `RLO_BEGIN`, `CE`, or `BSN`. \n",
    "\n",
    "An example of the content of these files is given by the following rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047a51f-5298-4de8-a83f-327fd018be27",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>B;0_474492234654248;0;COLLISION;23.667631;0:9.44621:18.0861:3:1:2.61393:1.21104:1:38.822:0.50739:19.1197:10.6772\n",
    "B;0_474492234654248;0;MERGER;23.667631;0:9.446e+00:1.906e+00:0.000e+00:3:0:18.1224:1:2.614e+00:0.000e+00:0.000e+00:1:0:1.21104:12.0601:38.822:0.50739\n",
    "S;0_474492234654248;0;NS;25.471686;5:1.46229:4.64623e+12:63.4538:0.98692\n",
    "S;0_474492234654248;0;SN;25.471686;10.6025:3.06259:1.65091:1.46229:5:2:453.262:345.701:-197.664:-212.483:-187.853\n",
    "S;0_641394535500269;0;WD;32.367569;3.2735:2.57868:1.29894:1.29894:3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52fd717",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing on the files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabece8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "In this section we are going to give a brief summary of the operations that we have to do on the files for our preprocessing. \n",
    "As anticipated, the final goal is to obtain a dataframe containing only the useful informations and save it for further analysis.\n",
    "\n",
    "In order to obtain only one dataframe we need to read, clean and extract informations from each of the three type of files and finally merge all the informations for each binary in a dataframe.\n",
    "\n",
    "\n",
    "All the preprocessing operations are written inside a function that is then executed in a delayed way with dask. \n",
    "\n",
    "For the preprocessing different attemps were made, and in the following section we are going to describe the pros and cons of each of them.\n",
    "\n",
    "Now we take as example one preprocessing function in order to describe the basic operations that we have done. \n",
    "The differences in the preprocessing function of each attempt will be highlighted in the corresponding sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d03d185",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# this function does the preprocessing\n",
    "# on three files of the same thread:\n",
    "# output_{thread}, evolved_{thread}, logfile_{thread}\n",
    "\n",
    "def preprocessing_bag_of_thread(paths):\n",
    "    '''\n",
    "       paths = python list of the paths of the three files \n",
    "       considered [output, evoleved, logfile]\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f132bab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    # list of column names and types to read\n",
    "\n",
    "    # output_{}.csv\n",
    "    output_column_to_read = ['name', 'Mass_0', 'RemnantType_0',\n",
    "                             'Mass_1', 'RemnantType_1',\n",
    "                             'Semimajor','Eccentricity',\n",
    "                             'GWtime','BWorldtime']\n",
    "    output_column_type = ['string', 'float64', 'int64',\n",
    "                          'float64', 'int64',\n",
    "                          'float64', 'float64',\n",
    "                          'float64', 'float64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0eab2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    # evolved_{}.dat\n",
    "    evolved_column_to_read = ['name', 'Mass_0',\n",
    "                              'Z_0', 'SN_0',\n",
    "                              'Mass_1', 'SN_1',\n",
    "                              'a', 'e']\n",
    "    evolved_column_type = ['string', 'float64',\n",
    "                           'float64', 'string',\n",
    "                           'float64', 'string',\n",
    "                           'float64', 'float64']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb9cec2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    # further columns to remove at the end \n",
    "    drop_list = ['RemnantType_0',  'RemnantType_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c312129",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    #OUTPUT files processing\n",
    "    \n",
    "    output = pd.read_csv(paths[0],                              # read the file\n",
    "                         usecols=output_column_to_read,         # read only some cols\n",
    "                         dtype=dict(zip(output_column_to_read,  # specify the types\n",
    "                                        output_column_type))).\\ #\n",
    "                rename(columns={'Mass_0':'Mass_0_out',          # rename columns\n",
    "                                'Mass_1':'Mass_1_out'})         #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d068696",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    # mask to select only the black holes binaries, defined by RemnantType\n",
    "    idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) &  (output.Semimajor.notnull())\n",
    "    \n",
    "    # apply the mask\n",
    "    output=output[idxBHBH]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b8d66",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    #EVOLVED files processing\n",
    "    \n",
    "    #extracting the alpha parameter from the path of the file \n",
    "    alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\",\n",
    "                             paths[1])[0])\n",
    "    \n",
    "    #read the columns we are interested in from the evolved file\n",
    "    evolved = pd.read_table(paths[1],                               # read file\n",
    "                            sep='\\s+',                              # separate by spaces\n",
    "                            usecols=evolved_column_to_read,         # read only some columns\n",
    "                            dtype=dict(zip(evolved_column_to_read,  # specify the types\n",
    "                                           evolved_column_type)))   #\n",
    "    #NB: sep='\\s+' is needed because there are different number of spaces\n",
    "    # separareting the columns\n",
    "    \n",
    "    #adding the column with the alpha parameter\n",
    "    evolved['alpha'] = alpha\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a2f06a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    #LOGFILE files processing\n",
    "    logfile = pd.read_csv(paths[2],    # read the file\n",
    "                          header=None) # there is no header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ce7e3d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    #Running Regex on the line of the logfile to extrac useful informations\n",
    "    df_RLO = logfile[0].\\\n",
    "           str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\ # searching string \"RLO_BEGIN\"\n",
    "           dropna().\\                                           # dropping nan\n",
    "           rename(columns={0:'name', 1:'ID'}).\\                 # rename columns\n",
    "           groupby(['name']).\\                                  # grouping by name\n",
    "           size().\\                                             # counting the number of RLO\n",
    "           to_frame(name='RLO').\\                               # \n",
    "           reset_index()                                        # to have a nice dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e436062c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "    df_CE = logfile[0].\\\n",
    "            str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\  # searching for string \"CE\"\n",
    "            dropna().\\                                     # dropping nan\n",
    "            rename(columns={0:'name', 1:'ID'}).\\           # rename\n",
    "            groupby(['name']).\\                            # grouping by name\n",
    "            size().\\                                       # \n",
    "            to_frame(name='CE').\\                          # and counting the number of CE\n",
    "            reset_index()                                  # to have a nice dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fb596",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    df_BSN = logfile[0].\\\n",
    "             str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\  #searching for string \"BSN\"\n",
    "             dropna().\\                                      # dropping nan\n",
    "             rename(columns={0:'name', 1:'ID'}).\\            #rename\n",
    "             groupby(['name']).\\                             #grouping by name\n",
    "             size().\\                                        #\n",
    "             to_frame(name='BSN').\\                          #and counting the number of BSN\n",
    "             reset_index()                                   #to have a nice dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9209824",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    #MERGE\n",
    "    bhbh = evolved.merge(output,           # inner join on the name \n",
    "                         on=['name'],      # between evolved and output\n",
    "                         how='inner').\\    #\n",
    "                   merge(df_RLO,           # left join on the name with df_RLO\n",
    "                         on=['name'],      #\n",
    "                         how='left').\\     #\n",
    "                   merge(df_CE,            # left join on the name with df_CE\n",
    "                         on=['name'],      #\n",
    "                         how='left').\\     #\n",
    "                   merge(df_BSN,           # left join on the name with df_BSN\n",
    "                         on=['name'],      #\n",
    "                         how='left').\\     #\n",
    "                   fillna(value=0).\\       # setting nan to zero\n",
    "                   drop(columns=drop_list) # dropping no longer useful columms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cf26b-be95-4a2a-bab5-469ca550d205",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    #Adding some columns with physical meaning\n",
    "    bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime'] #time delay\n",
    "    \n",
    "    #defining the max mass of output\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                            where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']),\n",
    "                                  other=bhbh['Mass_0_out'])\n",
    "\n",
    "    #defining q=m1/m2 with m2>,m1\n",
    "    bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "    bhbh['q'] = bhbh['q'].\\\n",
    "                where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']),\n",
    "                      other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "    \n",
    "    #defining the Chirp mass\n",
    "    bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))\n",
    "    \n",
    "    return bhbh # return the pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a347e6-9a98-4967-a8c6-5110dcf743de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Alternative solutions\n",
    "Here we present a summary of the different approaches we tried to tackle the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1822fe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing with no optimization\n",
    "#### The whole dataset all at once"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176782d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "The first attempt consists in loading the whole dataset (1.7TB) into three `dask.dataframe`: respectively all the output.csv, the evolved.dat and the logfile.dat files.\n",
    "\n",
    "This solution is the easiest to implement code wise but it is higly inefficient since it does not take advantage of the hierarchical structure of the data. \n",
    "Because of the cluster's memory limitation, the RAM fills up due to the shuffling operations as soon the merging operations begin.\n",
    "For this reason, this approach does not manage to process the entire volume of data and we have to serialize the operations over smaller batches.\n",
    "\n",
    "The code used for this solution differs from the one previously shown only in the way files are read and the columns on which the merges are carried out.\n",
    "The files are read all together by leveraging the wildcards capabilities of Dask, instead of using a Pandas DataFrame.\n",
    "The merges are performed on `name`, `Z_0` and `alpha` fileds as these three values uniquely identified the binary systems in the whole dataset.\n",
    "Even varying the number of partitions to higher numbers did not help with the high RAM usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac612f6-6c4e-459a-8e1b-59a3d213239e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd  # required to use the dask dataframes\n",
    "\n",
    "# omissis code\n",
    "\n",
    "# all files in all directories\n",
    "output = dd.read_csv('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/output_*.csv')  \n",
    "logfile = dd.read_csv('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/logfile_*.dat',\n",
    "                      header=None)\n",
    "evolved = dd.read_table('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/evolved_*.dat',\n",
    "                        sep='\\s+')\n",
    "\n",
    "# omissis code\n",
    "\n",
    "# note how the merges are performed\n",
    "bhbh = evolved.merge(output, on=['name','Z_0','alpha'], how='inner').\\ \n",
    "               merge(df_RLO, on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               merge(df_CE,  on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               merge(df_BSN, on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               fillna(value=0).\\\n",
    "               drop(columns=drop_list)\n",
    "\n",
    "# omissis code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fe55d-62d0-4024-9100-77eadbddae65",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Note that with the defaults values, Dask would create the following number of partitions for the whole dataset:\n",
    "* partitions for evolved files : 1800\n",
    "* partitions for output files : 21099\n",
    "* partitions for logfile files : 4350"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1626ad",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Folder and thread wise preproccessing\n",
    "#### Leveraging data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5491379-3069-4bc9-9f4e-e575ca6821f0",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In order to reduce the computational burden of the merge operations, we exploit the thread-wise organisation of the data by delaying the whole preprocessing over every triplet of files (output, evolved and logfile) in each folder.\n",
    "This is done by appending each task to a list and submitting it to the cluster to compute it.\n",
    "Even this approach fails to process the whole dateset all at once but we noticed it can analyse up to 300 GB at a time (~10 folders of data).\n",
    "\n",
    "The differences of the preprocessing code run are shown below. \n",
    "More explicitly: data is read folder by folder and thread by thread with dask dataframes.\n",
    "\n",
    "Notice there is an additional argument `n_part` and method `repartition` in the code.\n",
    "They are used only for benchmarking purposes and the first time the function was written we used the default number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26ced0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def FGpreprocessing_partitions(dir_path: str, n_thread: int,  # notice the explicit signature\n",
    "                               output_column_to_remove: list,\n",
    "                               evolved_column_to_remove: list,\n",
    "                               drop_list: list, n_part: int): \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfab17f-b02b-42a2-9d2e-1c485c1aa956",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "    # file names to read    \n",
    "    # now we read only one folder at a time\n",
    "    output_str = f'{dir_path}/0/output_{n_thread}.csv'    \n",
    "    evolved_str = f'{dir_path}/0/evolved_{n_thread}.dat'  \n",
    "    logfile_str = f'{dir_path}/0/logfile_{n_thread}.dat'  \n",
    "    \n",
    "   # omitted code    \n",
    "\n",
    "    output = dd.read_csv(output_str).\\\n",
    "                rename(columns={'Mass_0':'Mass_0_out',\n",
    "                                'Mass_1':'Mass_1_out'}).\\\n",
    "                drop(columns=output_column_to_remove).\\\n",
    "                repartition(npartitions = n_part)  \n",
    "     # note the possibility to repartition the output files\n",
    "     # it is used only for benchmarking purposes\n",
    "     # it was not used originally\n",
    "     # default number of partition was used instead \n",
    "    \n",
    "    # the following code is identical to the function above\n",
    "    # except for the use of dask dataframes instead of pandas dataframe\n",
    "    '''\n",
    "    dd.read_csv instead of pd.read_csv\n",
    "    '''\n",
    "    # the following code is therefore omitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bafb1b-5b58-4d6f-8a4f-d9f2d8afe4f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# this code is to properly map the function to the direcotries and threads\n",
    "\n",
    "n_threads = 30 # the number of thread to analyse\n",
    "bhbh_list=[]   # to store the delayed objects\n",
    "\n",
    "for dir_name in dir_list:         # scan some directories\n",
    "    for i in range(n_threads):    # scan the threads\n",
    "        _ = dask.delayed(FGpreprocessing_partitions)(dir_name, i,              # indexes to point to files\n",
    "                                                     output_column_to_remove,  # stuff to remove\n",
    "                                                     evolved_column_to_remove, #\n",
    "                                                     drop_list)                #\n",
    "        bhbh_list.append(_)       # save in the list\n",
    "        \n",
    "results = dask.compute(*bhbh_list) # trigger the computation\n",
    "results= dask.compute(*results)    # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20248a07-cbed-49a4-87fa-8525120db038",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For this approach we managed to carry out some benchmarks which are presented in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4a59a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Repartition by column 'name' the whole dataset 1.7TB\n",
    "#### No time wise benefits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53373c-6470-4c47-b72b-81ff20bd7242",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As we were able to see, the merging operations are quite RAM intensive to carry out, especially for very large chunks of data.\n",
    "According to the documentation, these operations can be sped up by using the dataframe `set_index` method, which explicitly tells dask how to address the data.\n",
    "\n",
    "In this approach we set the column `name` as the index of each dask dataframe.\n",
    "The dataframe contains every file of a specific kind for a a given folder. \n",
    "Then we repartition the dataframes such that each new partition contains only records with the same name.\n",
    "However, while the merge operations actually get faster, the whole setting index and repartitioning processes take so much time that by the end we do not see any benefits, time wise.\n",
    "Therefore we do not carry on with this strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44bab77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Serialization over the folders and repartion over name\n",
    "#### It works but takes 10 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db0ec6-b606-4574-b305-a43f8c009232",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Taking inspiration from the previous attempt, we leveraged on the data structure, namely the 60 folders.\n",
    "We process one folder at a time in a batch like system developed in a for cycle, processing 30GB at once.\n",
    "This serialization let us avoid processing the whole dataset of 1.7TB at once and reduces the computational weight of operations like `merge`, `reset_index` and `repartition` as they are carried out on fewer data, ~30GB.\n",
    "\n",
    "The whole task takes about 10 hours to fully run and data being saved to `parquet` files.\n",
    "Due to the time constraints this approach requires, it is was not benchmarked.\n",
    "\n",
    "The function used has the following form. \n",
    "We highlight only the main differences in the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86938552-aae7-4aea-95d1-7111b98706d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def FG_new(alpha: float, dir_path: str,      # signature with explicit types\n",
    "           output_column_to_remove: list,    #\n",
    "           evolved_column_to_remove: list,   #\n",
    "           drop_list: list):                 #\n",
    "    \n",
    "    output_str = f'{dir_path}/0/output_*.csv'   # read all threads at once\n",
    "    evolved_str = f'{dir_path}/0/evolved_*.dat' #\n",
    "    logfile_str = f'{dir_path}/0/logfile_*.dat' #\n",
    "\n",
    "    # omitted code to read the files and drop unnecessary columns\n",
    "    \n",
    "    # this codes actually performs the best repartition and division splitting\n",
    "    dask_divisions = output.set_index(\"name\").divisions\n",
    "    unique_divisions = list(dict.fromkeys(list(dask_divisions)))\n",
    "    \n",
    "    # omitted code\n",
    "    \n",
    "    output=output.set_index(\"name\", divisions=unique_divisions)\n",
    "    evolved = dd.read_table(evolved_str, sep='\\s+').\\\n",
    "                 drop(columns=evolved_column_to_remove).\\\n",
    "                 set_index(\"name\", divisions=unique_divisions)\n",
    "    \n",
    "    # actually perform the merges and \n",
    "    # the remaining computations\n",
    "    \n",
    "    return bhbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4813463-da04-4e23-9898-a8bfcaf2790a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# this code serializes the computation of the whole dataset\n",
    "\n",
    "bhbh_list=[]                                                     # the list to store the delayed objects\n",
    "for i,directory in enumerate(dir_list):                          # for all direcotries\n",
    "    bhbh_list.append(client.submit(FG_new, alpha[i], directory,  # submit these computations\n",
    "                                   output_column_to_remove,\n",
    "                                   evolved_column_to_remove,\n",
    "                                   drop_list))\n",
    "\n",
    "# compute and save its result\n",
    "for i in range(len(bhbh_list)):  # for each delayed object\n",
    "    bhbh_list[i].compute().to_parquet('.../direcotry_'+str(i)+'.parquet') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9fdf3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Thread wise preproccessing using Dask Bags\n",
    "#### It works great"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5261c48",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Still taking advantage of the thread-wise structure of the data, we change the paradigm of the task: we construct a Dask bag that contains one list for each triplet of threaded files inside the whole dataset. \n",
    "Given one of this lists, each element is the path to a particular kind of file (output, evolved or logfile), of the same thread."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28b7053-9a16-4608-8a28-02463c13cfa1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can now map the preprocessing function, which takes the files' paths as arguments, to each element of the bag and compute a single dataframe as a result. \n",
    "The benchmarks results are presented in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56850c9-46e0-45a6-b0b0-1aa8b81e69f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#bag of lists generated by a list comprehension\n",
    "bag=db.from_sequence([\n",
    "            [dir_ + f'/0/output_{thread}.csv', \n",
    "             dir_ + f'/0/evolved_{thread}.dat',\n",
    "             dir_ + f'/0/logfile_{thread}.dat'] for dir_ in dir_list for thread in range(30)],\n",
    "             npartitions=30*60)\n",
    "             # the number of partitions is set as such in order\n",
    "             # to have more or less one list per partition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e32c8",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>[[dir_0_output_0, dir_0_evolved_0, dir_0_logfile_0], [dir_0_output_1, dir_0_evolved_1, dir_0_logfile_1],...\n",
    "\n",
    "[dir_1_output_0, dir_1_evolved_0, dir_1_logfile_0], [dir_1_output_1, dir_1_evolved_1, dir_1_logfile_1],... ]</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea536f1-0cc6-4395-942b-66a3c63e0e84",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_bag_of_thread(paths):\n",
    "    \n",
    "    '''\n",
    "    The missing code is refered above \n",
    "    '''\n",
    "    \n",
    "    return bhbh #a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154dd98-2cee-45f0-aee3-f6d0388d4b83",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Map the preprocessing function to the bag\n",
    "bag_of_df = bag.map(preprocessing_bag_of_thread) \n",
    "\n",
    "#force a mapping of the bag to dictionaries in order to use the .to_dataframe() method\n",
    "bag_of_dicts = bag_of_df.map(lambda df: df.to_dict(orient='records')).flatten() \n",
    "\n",
    "#delayed function \n",
    "bhbh = bag_of_dicts.to_dataframe() \n",
    "\n",
    "#actual compute that returns the final pandas DataFrame\n",
    "bhbh = bhbh.compute() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664280cf-6fa7-4511-8c72-f7b79b1b941c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "This approach is quite efficient and manages to process the whole dataset in just 1 hour and 40 minutes. The final DataFrame, saved in the parquet format, is approximatly 1 GB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3dc06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2841b73b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* vary the number of workers, threads and partitions\n",
    "* repeat 5 times to obtain meaningful statistics\n",
    "* best-result to best-result approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38b0c4",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "In order to benchmark our preprocessing functions we decide to vary the number of workers, threads and partitions, testing this variations on a single directory of our dataset for a total of 5 times to obtain meaningful statistics. \n",
    "\n",
    "We decide to follow a best-result to best-result approach rather than a grid search strategy on the whole parameter space since this latter approach would be extremly time consuming. \n",
    "Firstly we test different couples (`number_of_workers`, `number_of_threads`), letting the partitions be taken automatically. \n",
    "Eventually we analyse the impact of varing the number of partitions that the cluster handles. \n",
    "\n",
    "For the data scalability test we evaluate the performances of our best combination of parameters on a varing number of directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438af6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "attempt = 5 #how many time we repeat the test to get meaningfull statistics \n",
    "\n",
    "#Lists of parameters for benchmark needs to be adjusted depending on the code\n",
    "workers_list = [1, 2, 4] \n",
    "threads_list = [1, 2, 4]\n",
    "partitions_list = [15, 30, 45] \n",
    "directory_list = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab078bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "time_list = np.zeros(shape=(attempt, \n",
    "                            len(partitions_list),\n",
    "                            len(workers_list),\n",
    "                            len(threads_list),\n",
    "                            len(directory_list))) #the benchmark data results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf412da",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for a in range(attempt):\n",
    "    for d in range(len(partitions_list)):\n",
    "        for w in range(len(workers_list)):\n",
    "            for t in range(len(threads_list)):\n",
    "                print('a:', a, '\\t','d:', partitions_list[d], '\\t', 'w:', workers_list[w], '\\t','t:', threads_list[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a7b6f0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "                #cluster configuration\n",
    "                cluster = SSHCluster([\"bhbh-1\", \"bhbh-1\", \"bhbh-2\", \"bhbh-3\", \"bhbh-4\", \"bhbh-5\"], #Machine IPaddres\n",
    "                                     connect_options={\"client_keys\": \"/path_to_private_key\"},      \n",
    "                                     worker_options={\"n_workers\": workers_list[w], \n",
    "                                                     \"nthreads\": threads_list[t]}, \n",
    "                                     scheduler_options={\"port\": 8786, \"dashboard_address\": \":8787\"})\n",
    "                client=Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296d6b78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "                time_i = time.time() # begin time\n",
    "                \n",
    "                '''Code of the proper function to be benchmarked '''\n",
    "                \n",
    "                time_f = time.time() # end time\n",
    "\n",
    "                #time difference allocation\n",
    "                time_list[a, d, w, t] = time_f - time_i\n",
    "\n",
    "                #cluster down\n",
    "                cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbffcd8-be24-4824-bd1b-803067da8763",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Save the result\n",
    "np.save(f'Benchmark_bag.npy', time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38ab42-622d-4899-81a5-f45bd462e43f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "##  Folder and thread wise preproccessing approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0afcf2-afb1-4ccc-b39a-266cf1007789",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]  |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]  |\n",
    "| Number of Partitions             | [1, 2, 12]  |\n",
    "| Number of Directory              | [1, 2, 3, 4]  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69464d-5c55-4781-9c1b-28caa86a3d3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Workers and Threads\n",
    "In this benchmark we vary the number of workers and threads per virtual machine, and we explore all the possible parameters combinations. We constrained the benchmark to the values reported in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae49cb-6adf-42dd-a56e-5933e9f613ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]  |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469e4cfa-195e-42b5-9217-67915e90978f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"Figures/bk_w_thr_old.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f79f6bd-3871-45d1-a26f-5d74d2945946",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The parameter configurartion that yields the best result is the one with 4 workers and 4 threads for each virtual machine, as shown in the heatmap above. This would be equal to have a 4 threaded virtual CPU for each worker. We can clearly see an improvement with the use of a larger number of workers, but also when this number is fixed, the time differences between thread choices are within the standard deviation uncertanties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192998e0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Partitions\n",
    "Selecting the parameters that lead to the best result in the previous section, in this benchmark we vary the number of partitions of the DataFrame for the `sevn output files`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa031c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "When using Dask DataFrame, Dask choice is to use 1 partition for the evolved files, 2 partitions for the logfile files, and 12 partitions for the output files, so we decide to vary this parameter by matching it to the automatic choices for the first two files, and check the results. The tested parameters are reported in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfadfd-fb7a-436b-87cc-2a59cc986e01",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Partitions | [1, 2, 12]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5a2cbc-4a65-45d0-b3ea-a2f6499c2cc5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"Figures/bk_divisions_old.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b59c9-83d6-41fb-ac86-5e14decee26d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Directories\n",
    "Selected the best model from the previous sections, we test the data scalability of our approach by varing the number of directory that are processed. \n",
    "\n",
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Directory              | [1, 2, 3, 4]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d059b-17b8-4a0d-b292-6100fb9a00c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src='Figures/bk_dir_old.png' width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d6f3f-5428-4502-9721-8bd56d0137f6",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In this case it seems like the algorithm cannot parallelize the workload much more than a directory at the time, since the time of the expected serialized model is compatible with the observed behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4d2d1-64b6-4154-8ca0-99ab92e23961",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Dask bag approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f5557-d5e2-4a59-bc65-fc48075c937d",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]     |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]     |\n",
    "| Number of Partitions             | [15, 30, 45]  |\n",
    "| Number of Directory              | [1, 2, 3, 4]  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25951-3bbc-48e3-b443-f7c80425d8d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Workers and Threads\n",
    "In this benchmark we vary the number of workers and thread per virtual machine, and we explore all the possible parameters combinations. We constrained the benchmark to the values reported in the table below:\n",
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]     |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331da2e5-b467-46bf-adbf-e4ae77381acb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"Figures/bk_w_thr_old_bag_try.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93155a42-a3bf-44be-af09-b632563f8721",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The parameter configurartion that yields the best result is the one with 4 workers and 2 threads for each virtual machine, as shown in the heatmap above. This would be equal to have a 2 threaded virtual CPU for each worker. We can clearly see an improvement with the use of a larger number of workers, but also when this number is fixed, the time differences between thread choices are within the standard deviation uncertanties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ca0fd-e91e-4474-9609-b298a1dc0d62",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Partitions\n",
    "Selecting the parameters that lead to the best result in the previous section, in this benchmark we vary the number of partitions of the input bag. Dask choice is to use 30 partitions so we decide to vary this number, by increasing and decreasing it, and check the results. The tested parameters are reported in the table below:\n",
    "\n",
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Partitions             | [15, 30, 45]  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967db66-7ee9-49bb-9d68-70205ff1153e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"Figures/bk_divisions_old_bag_try.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf15fe1-74b3-4435-9b1a-64de0dd09a9c",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "On average the dask choice yields the best results, even though all the results are compatible within the standard deviation uncertanties. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957eadd-aa59-452f-aa35-e19ab286e44b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Directory \n",
    "Selected the best model from the previous sections, we test the data scalability of our approach by varing the number of directory that are processed. \n",
    "\n",
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Directory              | [1, 2, 3, 4]  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46962eca-13de-4064-932e-493a05d5ecbd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"Figures/bk_dir_old_bag_try.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c7460a-964a-4b12-b8e8-8f4c103f05e1",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "As we can see the algorithm is faster than the serialized use of it on multiple directories, implying that this approach is able to parallelize more than 1 directory at the time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4ff8-ad89-420c-a7aa-47a1beef04a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading speed limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542963d-ebcc-4cf8-bc46-a07ce04d5900",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "To further analyse possible bottlenecks, we run a diagnostic on the network traffic sourcing from `bhbh-d`, while the cluster is crunching data.\n",
    "\n",
    "Since the data sharing infrastructure is build on a NFS server, we expect to see an high volume of traffic during the reading phase.\n",
    "To check if this is the case, we use the `iftop` diagnostic tool and, as can be seen from the picture below, there is a traffic of ~300MB/sec from `bhbh-d` to the other `bhbh-{1..5}`. \n",
    "Each `bhbh-{1..5}` reads ~60MB/sec and we noticed these rates vary in the same way: the cluster acts in a coordinated way.\n",
    "\n",
    "\n",
    "A final consideration on the time performace of the whole computation takes into account the network speed.\n",
    "A simple exchange of the whole dataset from the server `bhbh-d` to the `bhbh-{1..5}`, given the gross rates of 300MB/s, would just take around 1 hour and 35 minutes. \n",
    "This amount of time is comparable to the time required to actually carry out the computations which, as stated above, is 1 hour and 40 minutes.\n",
    "\n",
    "This insight suggests that to further reduce the overall speed, care should be taken on the way data itself is exchanged, since the whole process cannot be faster than a simple copy of the data among the workers.\n",
    "\n",
    "We need to remind that this method of sharing the files is limited by the capabilities of `bhbh-d` to serve data and strictly speaking is not a distributed file system.\n",
    "Data in reality resides only in one location, the volume `bhbh-v`, which is attached to a single machine; it is not scattered an replicated in each `bhbh-{1..5}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f6235",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center style=\"margin-left: 10%; margin-right: 10%; background-color: #eeeeee; padding-top: 10px; padding-bottom: 10px;\"><figure><img src=\"Figures/network_traffic.png\"><figcaption>Network traffic with the other VMs</figcaption></figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cabb4-63d8-4dba-96ef-b9fa8e61468e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center style=\"margin-left: 10%; margin-right: 10%; background-color: #eeeeee; padding-top: 10px; padding-bottom: 10px;\"><figure><img src=\"Figures/network_traffic_2.png\"><figcaption >Network traffic as a function of time</figcaption></figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcb61b-b683-411c-845d-13f3292acc40",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": []
   },
   "source": [
    "where {nthread} is a number ranging from 0 to 29, corresponding to the thread responsible for the computation of those simulations. \n",
    "\n",
    "On average, each of the `output_{nthread}.csv` files occupies 750MB, `logfile_{nthread}.dat` 200MB, and `evolved_{nthread}.dat` 50MB. \n",
    "\n",
    "In total, each folder occupies between 26 to 31GB of data for a gross total of around 1.7TB."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
