{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ace0c39",
   "metadata": {},
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>\n",
    "    <center>\n",
    "        <div style='  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;'>MAPD-B - Preprocessing of SEVN data for binary black holes mass distribution analysis\n",
    "        </div>\n",
    "    <center>\n",
    "        </br>\n",
    "    <i>Tommaso Bertola, Giacomo Di Prima, Giuseppe Viterbo, Marco Zenari</i></center></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab78bf",
   "metadata": {},
   "source": [
    "# Introduction: the computational problem\n",
    "\n",
    "Our aim for this project is to preprocess the data of multiple SEVN simulations of binary systems.\n",
    "\n",
    "SEVN is Python program developed by the Astronomy Department at the University of Padua to simulate the evolution of binary systems. The evolution takes into account different physical phenomena which obey the patterns and laws obeserved in the Universe, especially those seen in the stellar tracks.\n",
    "\n",
    "We will focus our attention specifically to those systems evolving into binary black holes. To study these systems we therefore need to extract from the whole dataset produced by SEVN only some information regarding the initial and final conditions of the binary system evolution.\n",
    "\n",
    "The final goal is to obtain a simple and handy DataFrame listing only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c77bc3",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "To better understading the problem, we will briefely describe the dataset we are given.\n",
    "\n",
    "The dataset consists of a number of folders named after some \"hyperparameters\" given to SEVN while performing the simulations.\n",
    "In our case, we are given 60 different folders, whose names are like `sevn_output_Z0.001A1L1`, `sevn_output_Z0.03A5L1`, ... More specifically the hyperparameters are the numbers following the letters `Z` and `L` which will be included in the final output DataFrame for each record.\n",
    "\n",
    "Inside each folder there are three kinds of files:\n",
    "* `output_{nthread}.csv`\n",
    "* `logfile_{nthread}.dat`\n",
    "* `evolved_{nthread}.dat`\n",
    "\n",
    "where {nthread} is a number ranging from 0 to 29, corresponding to the thread responsible for the computation of those simulations. \n",
    "\n",
    "On average, each of the `output_{nthread}.csv` files occupy 750MB, `logfile_{nthread}.dat` 200MB, and `evolved_{nthread}.dat` 50MB. \n",
    "\n",
    "In total, each folder occupies between 26 to 31GB of data for a gross total of around 1.7TB.\n",
    "\n",
    "\n",
    "## File schema\n",
    "We briefely report the schema of the three different kind of files to better explain the following parsing process\n",
    "\n",
    "### `evolved_{nthread}.dat` schema\n",
    "These files contain the initial properties of the systems that has been successfully evolved by SEVN.\n",
    "These are **fixed width** fields files and therefore we used the `pd.read_table` function to parse them.\n",
    "The fields we are interested in reading are reported below.\n",
    "\n",
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name|string `0_1234....`|Unique in each folder|\n",
    "|Mass_0|float|Initial mass of star 0 (in Solar masses)|\n",
    "|Mass_1|float|Initial mass of star 1 (in Solar masses)|\n",
    "|Z_0|float|Metallicity (both stars have the same)|\n",
    "|SN_0|string `rapid_gauNS`|Supernova model for star 0|\n",
    "|SN_1|string `rapid_gauNS`|Supernova model for star 1|\n",
    "|a|float| Initial semimajor of the binary |\n",
    "|e|float|Initial eccentricity of the binary|\n",
    "                      \n",
    "All other fields in the files are discarded in the analysis.\n",
    "\n",
    "\n",
    "### `output_{nthread}.csv` schema\n",
    "The output files contains the final conditions of the simulations.\n",
    "Each row is a different binary system, and each column indicates a different feature of the simulation.\n",
    "The file is formatted in a tipical **csv format**.\n",
    "We report in the following table the columns we are interested in. \n",
    "\n",
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name |string `0_1234...`| Unique in each folder|\n",
    "|Mass_0|double| Mass of object 0 (in Solar masses)|\n",
    "|Mass_1|double| Mass of object 1 (in Solar masses)|\n",
    "|RemnantType_0|int|Type of object 0 after evolution|\n",
    "|RemnantType_1|int|Type of object 1 after evolution|\n",
    "|Semimajor|float|Semimajor of the binary (in Solar radius)|\n",
    "|Eccentricity|float| Eccentricity of the binary|\n",
    "|GWtime|float|Gravitational wave orbital decay time|\n",
    "|BWorldtime|int|time elapsed in the simulations|\n",
    "\n",
    "All other fields in the files are discarded in the analysis.\n",
    "                      \n",
    "### `logfile_{nthread}.dat` schema\n",
    "Logfiles are **plain text** files, containing the description of a particular astrophysical event in each row.\n",
    "Each event is univocally associated to a specific binary system evolution by its `name`. \n",
    "To recover the relevant information we have to run different regular expressions.\n",
    "Each regex used specifically captures the name of the event, namely `RLO_BEGIN`, `CE`, or `BSN`. \n",
    "\n",
    "An example of the content of these files is given by the following rows\n",
    "\n",
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>B;0_474492234654248;0;COLLISION;23.667631;0:9.44621:18.0861:3:1:2.61393:1.21104:1:38.822:0.50739:19.1197:10.6772\n",
    "B;0_474492234654248;0;MERGER;23.667631;0:9.446e+00:1.906e+00:0.000e+00:3:0:18.1224:1:2.614e+00:0.000e+00:0.000e+00:1:0:1.21104:12.0601:38.822:0.50739\n",
    "S;0_474492234654248;0;NS;25.471686;5:1.46229:4.64623e+12:63.4538:0.98692\n",
    "S;0_474492234654248;0;SN;25.471686;10.6025:3.06259:1.65091:1.46229:5:2:453.262:345.701:-197.664:-212.483:-187.853\n",
    "S;0_641394535500269;0;WD;32.367569;3.2735:2.57868:1.29894:1.29894:3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00fc44",
   "metadata": {},
   "source": [
    "# Tools\n",
    "To parse the data we leverage on distributed computing techniques. \n",
    "Specifically we extensively use Python Dask library which enable us to manage a cluster of workers hosted on CloudVeneto virtual machines.\n",
    "\n",
    "## Infrastracture configuration\n",
    "![network_configuration.png](network_configuration.png)\n",
    "\n",
    "We will be using 6 different virtual machines, `bhbh-{d,1,2,3,4,5}`. \n",
    "The first, `bhbh-d`, serves as a NFS server, allowing all the other VMs `bhbh-{1,2,3,4,5}` to read, write and crunch and work with the archived data.\n",
    "\n",
    "### Setting up `bhbh-d`\n",
    "After instantiating the VM `bhbh-d` of flavor `cloudveneto.medium`, we attach the volume `bhbh-v` to `/dev/vdb` which is then mounted on `/mnt/bhbh-v`.\n",
    "To allow the other VMs to see the new volume, we use the software provided by `nfs-kernel-server`.\n",
    "\n",
    "The NFS server is configured such that `/mnt/bhbh-v` can be seen by the other VMs and therefore mounted therein.\n",
    "It is important to note that the configuration of the sharing of the volume is made in `sync` mode.\n",
    "The reason being the cluster might potentially alter the consistency of the data if `async` mode were used. \n",
    "\n",
    "Finally, the actual data were copied from the `demoblack.fisica.unipd.it` server to the mounted volume. The operation took a few hours as the total size of the data is around 1.7TB. The operation was carried out via `rsync` utility in order to resume the copying was the network to fail for some unspecified errors.\n",
    "\n",
    "### Setting up `bhbh-{1,2,3,4,5}`\n",
    "The VMs for the proper data crunching are of the flavour `cloudveneto.large`, having 8GB of RAM each for a total amount of 32GB<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1).\n",
    "The following operations are equally repeated on all the \n",
    "After installing the `nfs-utils` package, on\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) 32GB is comparable to the size of one folder.\n",
    "\n",
    "#### Particular set up of bhbh-1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabece8",
   "metadata": {},
   "source": [
    "# Preprocessing on the file\n",
    "\n",
    "In this section we are going to give a brief summary of the operations that we have to do on the file for our preprocessing. \n",
    "As anticipated, the final goal is to obtain a dataframe containing only the useful informations and save it for further analysis.\n",
    "\n",
    "In order to obtain only one dataframe we need to read, clean and extract informations from each of the three type of files and finally merge all the informations for each binary in a dataframe.\n",
    "All the preprocessing operations are written inside a function that is then executed in a delayd way with dask. \n",
    "\n",
    "For the preprocessing different attemps have been tried, and in the following section we are going to describe pro and cons of each of them. \n",
    "Now we take as example one preprocessing function in order to describe the basic operations that we have done. \n",
    "Differences in the preprocessing function of each attempt will be englighted in the corresponding section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c3999-c212-46ef-a34f-e08309837b33",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "#this function does the preprocessing on three files of the sam thread: output_{thread}, evolved_{thread}, logfile_{thread}\n",
    "\n",
    "def preprocessing_bag_of_thread(paths):\n",
    "    \n",
    "    '''\n",
    "       paths = list of the paths of the three file considered [output, evoleved, logfile]\n",
    "    '''\n",
    "    \n",
    "    #Listing the columns we are interested in the output file\n",
    "    output_column_to_read = ['name', 'Mass_0', 'RemnantType_0', 'Mass_1', 'RemnantType_1',\n",
    "                         'Semimajor','Eccentricity','GWtime','BWorldtime']\n",
    "    \n",
    "    #Listing the type of the column we are interested in\n",
    "    output_column_type = ['string', 'float64', 'int64', 'float64', 'int64',\n",
    "                      'float64', 'float64', 'float64', 'float64', 'int64']\n",
    "\n",
    "    #Listing the columns we are interested in the evolved file\n",
    "    evolved_column_to_read = ['name', 'Mass_0', 'Z_0', 'SN_0', 'Mass_1', 'SN_1', 'a', 'e']\n",
    "\n",
    "    #Listing the type of the column we are interested in\n",
    "    evolved_column_type = ['string', 'float64', 'float64', 'string', 'float64', \n",
    "                      'string', 'float64', 'float64']\n",
    "\n",
    "    #final columns to be dropped (they are used for filtering the dataset but will not be needed\n",
    "    drop_list = ['RemnantType_0',  'RemnantType_1']\n",
    "    \n",
    "   \n",
    "    #OUTPUT\n",
    "    \n",
    "    #reading the columns of the ouput we are interested in and renaming some of them\n",
    "    output = pd.read_csv(paths[0], usecols=output_column_to_read, dtype=dict(zip(output_column_to_read, output_column_type))).\\\n",
    "                rename(columns={'Mass_0':'Mass_0_out', 'Mass_1':'Mass_1_out'})\n",
    "\n",
    "    #mask to select only the black holes binaries\n",
    "    idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) & (output.Semimajor.notnull())\n",
    "    output=output[idxBHBH]\n",
    "        \n",
    "    \n",
    "    #EVOLVED\n",
    "    \n",
    "    #extracting the alpha parameter from the path of the folder\n",
    "    alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\", paths[1])[0])\n",
    "    \n",
    "    #read the columns we are interested in from the evolved file\n",
    "    evolved = pd.read_table(paths[1], sep='\\s+', usecols=evolved_column_to_read, dtype=dict(zip(evolved_column_to_read, evolved_column_type)))                \n",
    "    #NB: sep='\\s+' is need because there are different number of spaces separareting the columns\n",
    "    \n",
    "    #adding the column with the alpha parameter\n",
    "    evolved['alpha'] = alpha\n",
    "    \n",
    "    \n",
    "    #LOGFILE\n",
    "    \n",
    "    #reading the logfile \n",
    "    logfile = pd.read_csv(paths[2], header=None)\n",
    "\n",
    "    #Running Regex on the line of the logfile to extrac useful informations\n",
    "    \n",
    "    df_RLO = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\  #searching for string \"RLO_BEGIN\"\n",
    "                dropna().\\ # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\ #rename\n",
    "                groupby(['name']).\\ #grouping by name\n",
    "                size().to_frame(name='RLO').\\ #and counting the number of RLO\n",
    "                reset_index() #to have a nice dataframe\n",
    "\n",
    "    \n",
    "    df_CE = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\  #searching for string \"CE\"\n",
    "                dropna().\\  # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\ #rename\n",
    "                groupby(['name']).\\ #grouping by name\n",
    "                size().to_frame(name='CE').\\ #and counting the number of CE\n",
    "                reset_index() #to have a nice dataframe\n",
    "    \n",
    "\n",
    "    df_BSN = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\  #searching for string \"BSN\"\n",
    "                dropna().\\  # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\ #rename\n",
    "                groupby(['name']).\\ #grouping by name\n",
    "                size().to_frame(name='BSN').\\ #and counting the number of BSN\n",
    "                reset_index() #to have a nice dataframe\n",
    "\n",
    "    \n",
    "    #MERGE\n",
    "    bhbh = evolved.merge(output, on=['name'], how='inner').\\  #innerg join on the name between wvolved and output\n",
    "                   merge(df_RLO, on=['name'], how='left').\\   #left join on the name with df_RLO\n",
    "                   merge(df_CE,  on=['name'], how='left').\\   #left join on the name with df_CE\n",
    "                   merge(df_BSN, on=['name'], how='left').\\   #left join on the name with df_BSN\n",
    "                   fillna(value=0).\\   #setting nan to zero\n",
    "                   drop(columns=drop_list)  #dropping no more useful columm\n",
    "    \n",
    "    \n",
    "    #Adding some columns with physical meaning\n",
    "    bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime'] #time delay\n",
    "    \n",
    "    #defining the max mass of output\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                            where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']), other=bhbh['Mass_0_out'])\n",
    "\n",
    "    #defining q=m1/m2 with m2>,m1\n",
    "    bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "    bhbh['q'] = bhbh['q'].\\\n",
    "                where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']), other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "    \n",
    "    #defining the Chirp mass\n",
    "    bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))\n",
    "    \n",
    "    return bhbh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e396f54",
   "metadata": {},
   "source": [
    "# Our attempts\n",
    "Here we show the code to \n",
    "\n",
    "Load the whole thing at once\n",
    "\n",
    "FG_new\n",
    "\n",
    "FG_normal\n",
    "\n",
    "FG_bag\n",
    "\n",
    "Brute force approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4ff8-ad89-420c-a7aa-47a1beef04a2",
   "metadata": {},
   "source": [
    "## Possible improvements and limitations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
