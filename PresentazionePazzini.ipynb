{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ace0c39",
   "metadata": {},
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>\n",
    "    <center>\n",
    "        <div style='  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;'>MAPD-B - Preprocessing of SEVN data for binary black holes mass distribution analysis\n",
    "        </div>\n",
    "    <center>\n",
    "        </br>\n",
    "    <i>Tommaso Bertola, Giacomo Di Prima, Giuseppe Viterbo, Marco Zenari</i></center></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ab78bf",
   "metadata": {},
   "source": [
    "# Introduction: the computational problem\n",
    "\n",
    "Our aim for this project is to preprocess the data of multiple SEVN simulations of binary systems.\n",
    "\n",
    "SEVN is Python program developed by the Astronomy Department at the University of Padua to simulate the evolution of binary systems. The evolution takes into account different physical phenomena which obey the patterns and laws obeserved in the Universe, especially those seen in the stellar tracks.\n",
    "\n",
    "We will focus our attention specifically to those systems evolving into binary black holes. To study these systems we therefore need to extract from the whole dataset produced by SEVN only some information regarding the initial and final conditions of the binary system evolution.\n",
    "\n",
    "The final goal is to obtain a simple and handy DataFrame listing only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bcb61b-b683-411c-845d-13f3292acc40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data structure\n",
    "\n",
    "To better understading the problem, we will briefely describe the dataset we are given.\n",
    "\n",
    "The dataset consists of a number of folders named after some \"hyperparameters\" given to SEVN while performing the simulations.\n",
    "In our case, we are given 60 different folders, whose names are like `sevn_output_Z0.001A1L1`, `sevn_output_Z0.03A5L1`, ... More specifically the hyperparameters are the numbers following the letters `Z` and `L` which will be included in the final output DataFrame for each record.\n",
    "\n",
    "Inside each folder there are three kinds of files:\n",
    "* `output_{nthread}.csv`\n",
    "* `logfile_{nthread}.dat`\n",
    "* `evolved_{nthread}.dat`\n",
    "\n",
    "where {nthread} is a number ranging from 0 to 29, corresponding to the thread responsible for the computation of those simulations. \n",
    "\n",
    "On average, each of the `output_{nthread}.csv` files occupies 750MB, `logfile_{nthread}.dat` 200MB, and `evolved_{nthread}.dat` 50MB. \n",
    "\n",
    "In total, each folder occupies between 26 to 31GB of data for a gross total of around 1.7TB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8916aed-a4f0-4831-b92d-a0838ee01358",
   "metadata": {},
   "source": [
    "## File schema\n",
    "We describe the schema of the three different kind of files to better explain the following parsing process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a939d058-c06c-41d7-b34d-fd09027f27dc",
   "metadata": {},
   "source": [
    "### `evolved_{nthread}.dat` schema\n",
    "These files contain the initial properties of the systems that has been successfully evolved by SEVN.\n",
    "These are **fixed width** fields files and therefore we used the `pd.read_table` function to parse them.\n",
    "The fields we are interested in reading are reported below.\n",
    "\n",
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name|string `0_1234....`|Unique in each folder|\n",
    "|Mass_0|float|Initial mass of star 0 (in Solar masses)|\n",
    "|Mass_1|float|Initial mass of star 1 (in Solar masses)|\n",
    "|Z_0|float|Metallicity, same as in directory name (both stars have the same)|\n",
    "|SN_0|string `rapid_gauNS`|Supernova model for star 0|\n",
    "|SN_1|string `rapid_gauNS`|Supernova model for star 1|\n",
    "|a|float| Initial semimajor axis of the binary |\n",
    "|e|float|Initial eccentricity of the binary|\n",
    "                      \n",
    "All other fields in the files are discarded in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cc3707-efcd-415d-b263-efc839157972",
   "metadata": {},
   "source": [
    "### `output_{nthread}.csv` schema\n",
    "The output files contains the final conditions of the simulations.\n",
    "Each row is a different binary system, and each column indicates a different feature of the simulation.\n",
    "The file is formatted in a tipical **csv format**.\n",
    "We report in the following table the columns we are interested in. \n",
    "\n",
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name |string `0_1234...`| Unique in each folder|\n",
    "|Mass_0|double| Mass of object 0 (in Solar masses)|\n",
    "|Mass_1|double| Mass of object 1 (in Solar masses)|\n",
    "|RemnantType_0|int|Type of object 0 after evolution|\n",
    "|RemnantType_1|int|Type of object 1 after evolution|\n",
    "|Semimajor|float|Semimajor of the binary (in Solar radius)|\n",
    "|Eccentricity|float| Eccentricity of the binary|\n",
    "|GWtime|float|Gravitational wave orbital decay time|\n",
    "|BWorldtime|int|Time elapsed in the simulations|\n",
    "\n",
    "All other fields in the files are discarded in the analysis.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8047a51f-5298-4de8-a83f-327fd018be27",
   "metadata": {},
   "source": [
    "### `logfile_{nthread}.dat` schema\n",
    "Logfiles are **plain text** files, containing the description of a particular astrophysical event in each row.\n",
    "Each event is univocally associated to a specific binary system evolution by its `name`, however there could be multiple events regarding the same system.\n",
    "To recover the relevant information we have to run different regular expressions.\n",
    "Each regex used specifically captures the type of the event, namely `RLO_BEGIN`, `CE`, or `BSN`. \n",
    "\n",
    "An example of the content of these files is given by the following rows\n",
    "\n",
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>B;0_474492234654248;0;COLLISION;23.667631;0:9.44621:18.0861:3:1:2.61393:1.21104:1:38.822:0.50739:19.1197:10.6772\n",
    "B;0_474492234654248;0;MERGER;23.667631;0:9.446e+00:1.906e+00:0.000e+00:3:0:18.1224:1:2.614e+00:0.000e+00:0.000e+00:1:0:1.21104:12.0601:38.822:0.50739\n",
    "S;0_474492234654248;0;NS;25.471686;5:1.46229:4.64623e+12:63.4538:0.98692\n",
    "S;0_474492234654248;0;SN;25.471686;10.6025:3.06259:1.65091:1.46229:5:2:453.262:345.701:-197.664:-212.483:-187.853\n",
    "S;0_641394535500269;0;WD;32.367569;3.2735:2.57868:1.29894:1.29894:3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e037bf-eb7a-432a-8d72-c897a5bbf0ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tools\n",
    "To parse the data we leverage on distributed computing techniques. \n",
    "Specifically we extensively use Python Dask library which enable us to manage a cluster of workers hosted on CloudVeneto virtual machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06672cd-a3fd-421f-a387-17a69531a807",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Infrastracture configuration\n",
    "![network_configuration.png](network_configuration.png)\n",
    "\n",
    "We will be using 6 different virtual machines, `bhbh-{d,1,2,3,4,5}`. \n",
    "The first, `bhbh-d`, serves as a NFS server, allowing all the other VMs `bhbh-{1,2,3,4,5}` to read, write and crunch the archived data as can be seen from the picture above with the green arrows.\n",
    "Highlighted in orange, all `bhbh-{1..5}` are connected to a Dask cluster, managed solely by `bhbh-1` which performs the action of client, scheduler and worker at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1b901-5faa-4d35-abc5-fc1ba353f0f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up `bhbh-d`\n",
    "After instantiating the VM `bhbh-d` of flavor `cloudveneto.medium`, we attach the volume `bhbh-v` to `/dev/vdb` which is then mounted on `/mnt/bhbh-v`.\n",
    "To allow the other VMs to see the new volume, we use the software provided by `nfs-kernel-server`.\n",
    "\n",
    "The NFS server is configured such that `/mnt/bhbh-v` can be seen by the other VMs and therefore mounted therein.\n",
    "It is important to note that the configuration of the sharing of the volume is made in `sync` mode.\n",
    "The reason being the cluster might potentially alter the consistency of the data if `async` mode were used. \n",
    "\n",
    "Finally, the actual data were copied from the `demoblack.fisica.unipd.it` server to the mounted volume.\n",
    "The operation took a few hours as the total size of the data is around 1.7TB.\n",
    "The operation was carried out via `rsync` utility in order to resume the copying was the network to fail for some unspecified errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e58dcb-ad4a-48df-a54c-2b50641d5c72",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up `bhbh-{1,2,3,4,5}`\n",
    "The VMs for the proper data crunching are of the flavour `cloudveneto.large`, having 8GB of RAM each for a total amount of 32GB<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1).\n",
    "The following operations are equally repeated on all the VMs in order to have equally working systems.\n",
    "\n",
    "After installing the `nfs-utils` package, we mount the Network volume issuing the following command `mount -t nfs 10.67.22.169:/mnt/bhbh-v /mnt/bhbh`. \n",
    "For simplicty sake we use `conda` as a package manager. We install the `dask[complete]` suite of packages.\n",
    "This set up is enought to allow the cluster to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e350242-568e-4a5c-b513-dee2f33ab5fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Particular set up of bhbh-1\n",
    "The `bhbh-1` has some slight differences in the software installed as it is used for different puropses.\n",
    "`bhbh-1` hosts the Jupyter server, acting as a client to the cluster, the cluster scheduler and also a cluster worker.\n",
    "The reason why this set up is used is just for optimizing the memory and computing resources available.\n",
    "A better configuration would have been achieved by moving the client and the scheduler to another VM but this was not easy to do as cloud resources were limited.\n",
    "Indeed, one of the main issues we came across during the tests was a lack of RAM in the `bhbh-1`, presumably due to the exchange of data between workers. For this reason we are discussing different approaches in the following.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) 32GB is comparable to the size of one folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fabece8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing on the files\n",
    "\n",
    "In this section we are going to give a brief summary of the operations that we have to do on the files for our preprocessing. \n",
    "As anticipated, the final goal is to obtain a dataframe containing only the useful informations and save it for further analysis.\n",
    "\n",
    "In order to obtain only one dataframe we need to read, clean and extract informations from each of the three type of files and finally merge all the informations for each binary in a dataframe.\n",
    "\n",
    "\n",
    "All the preprocessing operations are written inside a function that is then executed in a delayd way with dask. \n",
    "\n",
    "For the preprocessing different attemps have been tried, and in the following section we are going to describe the pros and cons of each of them.\n",
    "\n",
    "Now we take as example one preprocessing function in order to describe the basic operations that we have done. \n",
    "The differences in the preprocessing function of each attempt will be highlighted in the corresponding sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149cf26b-be95-4a2a-bab5-469ca550d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function does the preprocessing\n",
    "# on three files of the same thread:\n",
    "# output_{thread}, evolved_{thread}, logfile_{thread}\n",
    "\n",
    "def preprocessing_bag_of_thread(paths):\n",
    "    '''\n",
    "       paths = python list of the paths of the three files considered [output, evoleved, logfile]\n",
    "    '''\n",
    "    \n",
    "    # list of column names and types to read\n",
    "    \n",
    "    # output_{}.csv\n",
    "    output_column_to_read = ['name', 'Mass_0', 'RemnantType_0',\n",
    "                             'Mass_1', 'RemnantType_1',\n",
    "                             'Semimajor','Eccentricity',\n",
    "                             'GWtime','BWorldtime']\n",
    "    output_column_type = ['string', 'float64', 'int64',\n",
    "                          'float64', 'int64',\n",
    "                          'float64', 'float64',\n",
    "                          'float64', 'float64']\n",
    "\n",
    "    # evolved_{}.dat\n",
    "    evolved_column_to_read = ['name', 'Mass_0',\n",
    "                              'Z_0', 'SN_0',\n",
    "                              'Mass_1', 'SN_1',\n",
    "                              'a', 'e']\n",
    "    evolved_column_type = ['string', 'float64',\n",
    "                           'float64', 'string',\n",
    "                           'float64', 'string',\n",
    "                           'float64', 'float64']\n",
    "    \n",
    "    # further columns to remove at the end \n",
    "    drop_list = ['RemnantType_0',  'RemnantType_1']\n",
    "    \n",
    "   \n",
    "    #OUTPUT files processing\n",
    "    \n",
    "    output = pd.read_csv(paths[0],                              # read the file\n",
    "                         usecols=output_column_to_read,         # read only some cols\n",
    "                         dtype=dict(zip(output_column_to_read,  # specify the types\n",
    "                                        output_column_type))).\\ #\n",
    "                rename(columns={'Mass_0':'Mass_0_out',          # rename columns\n",
    "                                'Mass_1':'Mass_1_out'})         #\n",
    "\n",
    "    # mask to select only the black holes binaries, defined by RemnantType\n",
    "    idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) & (output.Semimajor.notnull())\n",
    "    \n",
    "    # apply the mask\n",
    "    output=output[idxBHBH]\n",
    "        \n",
    "    \n",
    "    #EVOLVED files processing\n",
    "    \n",
    "    #extracting the alpha parameter from the path of the file \n",
    "    alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\",\n",
    "                             paths[1])[0])\n",
    "    \n",
    "    #read the columns we are interested in from the evolved file\n",
    "    evolved = pd.read_table(paths[1],                               # read file\n",
    "                            sep='\\s+',                              # separate by spaces\n",
    "                            usecols=evolved_column_to_read,         # read only some columns\n",
    "                            dtype=dict(zip(evolved_column_to_read,  # specify the types\n",
    "                                           evolved_column_type)))   #\n",
    "    #NB: sep='\\s+' is need because there are different number of spaces separareting the columns\n",
    "    \n",
    "    #adding the column with the alpha parameter\n",
    "    evolved['alpha'] = alpha\n",
    "    \n",
    "    \n",
    "    #LOGFILE files processing\n",
    "    \n",
    "    logfile = pd.read_csv(paths[2],    # read the file\n",
    "                          header=None) # there is no header\n",
    "\n",
    "    \n",
    "    #Running Regex on the line of the logfile to extrac useful informations\n",
    "    df_RLO = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\  # searching for string \"RLO_BEGIN\"\n",
    "                dropna().\\                                                    # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\                          # rename columns\n",
    "                groupby(['name']).\\                                           # grouping by name\n",
    "                size().\\                                                      # and counting the number of RLO\n",
    "                to_frame(name='RLO').\\                                        # \n",
    "                reset_index()                                                 # to have a nice dataframe\n",
    "\n",
    "    \n",
    "    df_CE = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\  # searching for string \"CE\"\n",
    "                dropna().\\                                            # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\                  # rename\n",
    "                groupby(['name']).\\                                   # grouping by name\n",
    "                size().\\                                              # \n",
    "                to_frame(name='CE').\\                                 # and counting the number of CE\n",
    "                reset_index()                                         # to have a nice dataframe\n",
    "    \n",
    "\n",
    "    df_BSN = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\  #searching for string \"BSN\"\n",
    "                dropna().\\                                              # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\                    #rename\n",
    "                groupby(['name']).\\                                     #grouping by name\n",
    "                size().\\                                                #\n",
    "                to_frame(name='BSN').\\                                  #and counting the number of BSN\n",
    "                reset_index()                                           #to have a nice dataframe\n",
    "\n",
    "    \n",
    "    #MERGE\n",
    "    bhbh = evolved.merge(output, on=['name'], how='inner').\\  #innerg join on the name between wvolved and output\n",
    "                   merge(df_RLO, on=['name'], how='left').\\   #left join on the name with df_RLO\n",
    "                   merge(df_CE,  on=['name'], how='left').\\   #left join on the name with df_CE\n",
    "                   merge(df_BSN, on=['name'], how='left').\\   #left join on the name with df_BSN\n",
    "                   fillna(value=0).\\                          #setting nan to zero\n",
    "                   drop(columns=drop_list)                    #dropping no longer useful columms\n",
    "    \n",
    "    \n",
    "    #Adding some columns with physical meaning\n",
    "    bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime'] #time delay\n",
    "    \n",
    "    #defining the max mass of output\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                            where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']),\n",
    "                                  other=bhbh['Mass_0_out'])\n",
    "\n",
    "    #defining q=m1/m2 with m2>,m1\n",
    "    bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "    bhbh['q'] = bhbh['q'].\\\n",
    "                where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']),\n",
    "                      other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "    \n",
    "    #defining the Chirp mass\n",
    "    bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))\n",
    "    \n",
    "    return bhbh # return the pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a347e6-9a98-4967-a8c6-5110dcf743de",
   "metadata": {},
   "source": [
    "# Alternative solutions\n",
    "Here we present a summary of the different approaches we tried to tackle the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fe55d-62d0-4024-9100-77eadbddae65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing with no optimization - the whole dataset all at once\n",
    "The first attempt consists in loading the whole dataset (1.7TB) into three `dask.dataframe`: respectively all the output.csv, the evolved.dat and the logfile.dat files.\n",
    "\n",
    "This solution is the easiest to implement code wise but it is higly inefficient since it does not take advantage of the hierarchical structure of the data. \n",
    "Because of the cluster's memory limitation, the RAM fills up due to the shuffling operations as soon the merging operations begin.\n",
    "For this reason, this approach does not manage to process the entire volume of data and we have to serialize the operations over smaller batches.\n",
    "\n",
    "The code used for this solution differs from the one previously shown only in the way files are read and the columns on hich the merges are carried out.\n",
    "The files are read all together by leveraging the wildcards capabilities of Dask, instead of using a Pandas DataFrame.\n",
    "The merges are performed on `name`, `Z_0` and `alpha` fileds as these three values uniquely identified the binary systems in the whole dataset.\n",
    "Even varying the number of partitions to higher numbers did not help with the high RAM usage.\n",
    "\n",
    "Note that with the defaults values, Dask would create the following number of partitions for the whole dataset:\n",
    "* partitions for evolved files : 1800\n",
    "* partitions for output files : 21099\n",
    "* partitions for logfile files : 4350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac612f6-6c4e-459a-8e1b-59a3d213239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd  # required to use the dask dataframes\n",
    "\n",
    "# omissis code\n",
    "\n",
    "output = dd.read_csv('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/output_*.csv')   # all files in all directories\n",
    "logfile = dd.read_csv('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/logfile_*.dat', header=None)\n",
    "evolved = dd.read_table('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/evolved_*.dat', sep='\\s+')\n",
    "\n",
    "# omissis code\n",
    "\n",
    "bhbh = evolved.merge(output, on=['name','Z_0','alpha'], how='inner').\\ # note how the merges are performed\n",
    "               merge(df_RLO, on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               merge(df_CE,  on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               merge(df_BSN, on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               fillna(value=0).\\\n",
    "               drop(columns=drop_list)\n",
    "\n",
    "# omissis code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5491379-3069-4bc9-9f4e-e575ca6821f0",
   "metadata": {},
   "source": [
    "## Folder and thread wise preproccessing\n",
    "In order to reduce even further the computational burden of the merge operations, we exploit the thread-wise organisation of the data by delaying the whole preprocessing over every triplet of files (output, evolved and logfile) in each folder.\n",
    "This is done by appending each task to a list and submitting it to the cluster to compute it.\n",
    "Even this approach fails to process the whole dateset all at once but we noticed it can analyse 300 GB at a time (~10 folders of data).\n",
    "\n",
    "The differences of the preprocessing code run are shown below. \n",
    "More explicitly: data is read folder by folder and thread by thread with dask dataframes.\n",
    "\n",
    "Notice there is an additional argument `n_part` and method `repartition` in the code.\n",
    "They are used only for benchmarking purposes and the first time the function was written we used the default number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfab17f-b02b-42a2-9d2e-1c485c1aa956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGpreprocessing_partitions(dir_path: str, n_thread: int,  # notice the explicit signature\n",
    "                               output_column_to_remove: list,\n",
    "                               evolved_column_to_remove: list,\n",
    "                               drop_list: list, n_part: int): \n",
    "    \n",
    "    # file names to read    \n",
    "    output_str = f'{dir_path}/0/output_{n_thread}.csv'    # now we read only one folder at a time\n",
    "    evolved_str = f'{dir_path}/0/evolved_{n_thread}.dat'  #\n",
    "    logfile_str = f'{dir_path}/0/logfile_{n_thread}.dat'  #\n",
    "    \n",
    "   # omitted code    \n",
    "\n",
    "    output = dd.read_csv(output_str).\\\n",
    "                rename(columns={'Mass_0':'Mass_0_out',\n",
    "                                'Mass_1':'Mass_1_out'}).\\\n",
    "                drop(columns=output_column_to_remove).\\\n",
    "                repartition(npartitions = n_part)            # note the possibility to repartition the output files\n",
    "                                                             # it is used only for benchmarking purposes\n",
    "                                                             # it was not used originally\n",
    "                                                             # default number of partition was used instead \n",
    "    \n",
    "    # the following code is identical to the function above\n",
    "    # except for the use of dask dataframes instead of pandas dataframe\n",
    "    '''\n",
    "    dd.read_csv instead of pd.read_csv\n",
    "    '''\n",
    "    # the following code is therefore omitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bafb1b-5b58-4d6f-8a4f-d9f2d8afe4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is to properly map the function to the direcotries and threads\n",
    "\n",
    "n_threads = 30 # the number of thread to analyse\n",
    "bhbh_list=[]   # to store the delayed objects\n",
    "\n",
    "for dir_name in dir_list:         # scan some directories\n",
    "    for i in range(n_threads):    # scan the threads\n",
    "        _ = dask.delayed(FGpreprocessing_partitions)(dir_name, i,              # indexes to point to files\n",
    "                                                     output_column_to_remove,  # stuff to remove\n",
    "                                                     evolved_column_to_remove, #\n",
    "                                                     drop_list)                #\n",
    "        bhbh_list.append(_)       # save in the list\n",
    "        \n",
    "results = dask.compute(*bhbh_list) # trigger the computation\n",
    "results= dask.compute(*results)    # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20248a07-cbed-49a4-87fa-8525120db038",
   "metadata": {},
   "source": [
    "For this approach we managed to carry out some benchmarks which are presented in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53373c-6470-4c47-b72b-81ff20bd7242",
   "metadata": {},
   "source": [
    "## Repartition by column 'name' the whole dataset 1.7TB - no time wise benefits\n",
    "As we were able to see, the merging operations are quite RAM intensive to carry out, especially for very large chunks of data.\n",
    "According to the documentation, these operations can be sped up by using the dataframe `set_index` method, which explicitly tells dask how to address the data.\n",
    "\n",
    "In this approach we set the column `name` as the index of each dask dataframe.\n",
    "The dataframe contains every file of a specific kind for a a given folder. \n",
    "Then we repartition the dataframes such that each new partition contains only records with the same name.\n",
    "However, while the merge operations actually get faster, the whole setting index and repartitioning processes take so much time that by the end we do not see any benefits, time wise.\n",
    "Therefore we do not carry on with this strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db0ec6-b606-4574-b305-a43f8c009232",
   "metadata": {},
   "source": [
    "## Serialization over the folders and repartion over name - it works but takes 10 hours\n",
    "Taking inspiration from the previous attempt, we leveraged on the data structure, namely the 60 folders.\n",
    "We process one folder at a time in a batch like system developed in a for cycle, processing 30GB at once.\n",
    "This serialization let us avoid processing the whole dataset of 1.7TB at once and reduces the computational weight of operations like `merge`, `reset_index` and `repartition` as they are carried out on fewer data, ~30GB.\n",
    "\n",
    "The whole task takes about 10 hours to fully run and being saved to `parquet` files.\n",
    "Due to the time constraints this approach requires, it is was not benchmarked.\n",
    "\n",
    "The function used has the following form. \n",
    "We highlight only the main differences in the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86938552-aae7-4aea-95d1-7111b98706d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FG_new(alpha: float, dir_path: str,      # signature with explicit types\n",
    "           output_column_to_remove: list,    #\n",
    "           evolved_column_to_remove: list,   #\n",
    "           drop_list: list):                 #\n",
    "    \n",
    "    output_str = f'{dir_path}/0/output_*.csv'   # read all threads at once\n",
    "    evolved_str = f'{dir_path}/0/evolved_*.dat' #\n",
    "    logfile_str = f'{dir_path}/0/logfile_*.dat' #\n",
    "\n",
    "    # omitted code to read the files and drop unnecessary columns\n",
    "    \n",
    "    dask_divisions = output.set_index(\"name\").divisions                    # this codes actually performs the best repartition and division splitting\n",
    "    unique_divisions = list(dict.fromkeys(list(dask_divisions)))\n",
    "    \n",
    "    # omitted code\n",
    "    # actually perform the merges and \n",
    "    # the remaining computations\n",
    "    \n",
    "    return bhbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4813463-da04-4e23-9898-a8bfcaf2790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code serializes the computation of the while dataset\n",
    "\n",
    "bhbh_list=[]                                                     # the list to store the delayed objects\n",
    "for i,directory in enumerate(dir_list):                          # for all direcotries\n",
    "    bhbh_list.append(client.submit(FG_new,alpha[i], directory,   # submit these computations\n",
    "                                   output_column_to_remove,\n",
    "                                   evolved_column_to_remove,\n",
    "                                   drop_list))\n",
    "\n",
    "for i in range(len(bhbh_list)):  # for each delayed object\n",
    "    bhbh_list[i].compute().to_parquet('/mnt/bhbh/folder_by_folder_repartition/direcotry_'+str(i)+'.parquet') # compute and save its result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ba3db-1cea-4ee2-b6d8-cbfa0f4ae587",
   "metadata": {},
   "source": [
    "## Thread wise preproccessing using Dask Bags\n",
    "Still takeing advantage of the thread-wise structure of the data, we change the paradigm of the task: we construct a Dask bag that contains one list for each triplet of threaded files inside the whole dataset. \n",
    "Given one of this lists, each element is the path to a particular kind of file (output, evolved or logfile), with the same thread. \n",
    "We can now apply the preprocessing function, which takes the files' paths as arguments, to each element of the bag and compute a single dataframe as a result by casting the whole bag as a dictionary and than to a dataframe.\n",
    "The benchmarks results are presented in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea536f1-0cc6-4395-942b-66a3c63e0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_bag_of_thread(paths):\n",
    "    \n",
    "    #lists of columns to read for each file and corresponding type\n",
    "    output_column_to_read = ['name', 'Mass_0', 'RemnantType_0', 'Mass_1', 'RemnantType_1',\n",
    "                         'Semimajor','Eccentricity','GWtime','BWorldtime']\n",
    "\n",
    "    output_column_type = ['string', 'float64', 'int64', 'float64', 'int64',\n",
    "                      'float64', 'float64', 'float64', 'float64']\n",
    "\n",
    "    evolved_column_to_read = ['name', 'Mass_0', 'Z_0', 'SN_0', 'Mass_1', 'SN_1', 'a', 'e']\n",
    "\n",
    "\n",
    "    evolved_column_type = ['string', 'float64', 'float64', 'string', 'float64', \n",
    "                      'string', 'float64', 'float64']\n",
    "\n",
    "    drop_list = ['RemnantType_0',  'RemnantType_1']\n",
    "    \n",
    "   \n",
    "    #Preprocessing OUTPUT\n",
    "    \n",
    "    #reading the file\n",
    "    output = pd.read_csv(paths[0], usecols=output_column_to_read, dtype=dict(zip(output_column_to_read, output_column_type))).\\\n",
    "                rename(columns={'Mass_0':'Mass_0_out', 'Mass_1':'Mass_1_out'})\n",
    "    \n",
    "    #mask to select only the binaries we are interested in\n",
    "    idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) & (output.Semimajor.notnull())\n",
    "    output=output[idxBHBH]    \n",
    "    \n",
    "    \n",
    "    #preprocessing EVOLVED\n",
    "      \n",
    "    #reading the file\n",
    "    evolved = pd.read_table(paths[1], sep='\\s+', usecols=evolved_column_to_read, dtype=dict(zip(evolved_column_to_read, evolved_column_type)))                \n",
    "    \n",
    "    #extracting alpha with a regex\n",
    "    alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\", paths[1])[0])\n",
    "    evolved['alpha'] = alpha\n",
    "    \n",
    "    \n",
    "    #preprocessing LOGFILE\n",
    "    \n",
    "    logfile = pd.read_csv(paths[2], header=None)\n",
    "    \n",
    "    \n",
    "    #extracting informations with regex\n",
    "\n",
    "    df_RLO = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\\n",
    "                dropna().\\\n",
    "                rename(columns={0:'name', 1:'ID'}).\\\n",
    "                groupby(['name']).\\\n",
    "                size().to_frame(name='RLO').\\\n",
    "                reset_index()\n",
    "\n",
    "    df_CE = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\\n",
    "                dropna().\\\n",
    "                rename(columns={0:'name', 1:'ID'}).\\\n",
    "                groupby(['name']).\\\n",
    "                size().to_frame(name='CE').\\\n",
    "                reset_index()\n",
    "\n",
    "    df_BSN = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\\n",
    "                dropna().\\\n",
    "                rename(columns={0:'name', 1:'ID'}).\\\n",
    "                groupby(['name']).\\\n",
    "                size().to_frame(name='BSN').\\\n",
    "                reset_index()\n",
    "\n",
    "    \n",
    "    #MERGE\n",
    "    bhbh = evolved.merge(output, on=['name'], how='inner').\\\n",
    "                   merge(df_RLO, on=['name'], how='left').\\\n",
    "                   merge(df_CE,  on=['name'], how='left').\\\n",
    "                   merge(df_BSN, on=['name'], how='left').\\\n",
    "                   fillna(value=0).\\\n",
    "                   drop(columns=drop_list)\n",
    "    \n",
    "    \n",
    "    #add some useful columns\n",
    "    bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime']\n",
    "\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                            where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']), other=bhbh['Mass_0_out'])\n",
    "\n",
    "    bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "    bhbh['q'] = bhbh['q'].\\\n",
    "                where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']), other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "\n",
    "    bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))\n",
    "    \n",
    "    \n",
    "    return bhbh #a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d253b048-79bf-43e9-8b2e-a311eb3d1ad4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmarks\n",
    "In order to benchmark our preprocessing functions we decide to vary the number of workers, threads and partitions, testing this variations on a single directory of our dataset for a total of 5 times to obtain meaningfull statistics.\n",
    "Also, we try to see if there is any benefit in using the `multiprocessing` options for the workers, instead of the default `distributed`. We decide to follow a best-result to best-result approach rather than a grid search strategy on the whole parameter space since this latter approach would be extremly time consuming. \n",
    "Firstly we test different couples (`number_of_workers`, `number_of_threads`), letting the partitions be taken automatically. \n",
    "Using the couple of paramteres that took the least amount of time, we assess the effect of the `multiprocessing` option for the workers. \n",
    "Eventually we analyse the impact of varing the number of partitions that the cluster handles. \n",
    "For the data scalability test we evaluate the performance of our best combination of parameters on a varing number of directories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf4d2d1-64b6-4154-8ca0-99ab92e23961",
   "metadata": {},
   "source": [
    "## FG_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785f5557-d5e2-4a59-bc65-fc48075c937d",
   "metadata": {},
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]  |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]  |\n",
    "| Workers option                   | [`Multiprocessing`, `Distributed`] |\n",
    "| Number of Partitions             | TOBEDEFINED |\n",
    "| Number of Directory              | [1, 2, 3, 4]  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbffcd8-be24-4824-bd1b-803067da8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = 5\n",
    "workers_list = [1, 2, 4]\n",
    "threads_list = [1, 2, 4]\n",
    "partitions_list = [1, 12, 24] \n",
    "time_list = np.zeros(shape=(attempt, len(partitions_list), len(workers_list), len(threads_list)))\n",
    "multiproccesing_method = False\n",
    "\n",
    "for a in range(attempt):\n",
    "    for d in range(len(partitions_list)):\n",
    "        for w in range(len(workers_list)):\n",
    "            for t in range(len(threads_list)):\n",
    "                print('a:', a, '\\t','d:', partitions_list[d], '\\t', 'w:', workers_list[w], '\\t','t:', threads_list[t])\n",
    "\n",
    "                #cluster up\n",
    "                cluster = SSHCluster(\n",
    "                [\"bhbh-1\", \"bhbh-1\", \"bhbh-2\", \"bhbh-3\", \"bhbh-4\", \"bhbh-5\"],\n",
    "                connect_options={\"client_keys\": \"/home/ubuntu/private/tbertola_key.pem\"},\n",
    "                worker_options={\"n_workers\": workers_list[w],\n",
    "                                \"nthreads\": threads_list[t]}, # because each bhbh-* has 4 cores\n",
    "                scheduler_options={\"port\": 8786, \"dashboard_address\": \":8787\"}\n",
    "                                    )\n",
    "                client=Client(cluster)\n",
    "                \n",
    "                #Configure workers as multiprocessing instead of distributed (default option)\n",
    "                if multiproccesing_method == True:\n",
    "                    dask.config.set({'distributed.worker.multiprocessing-method': 'spawn'})\n",
    "\n",
    "                #begin time\n",
    "                time_i = time.time()\n",
    "                \n",
    "                #input bag for the 'preprocessing_bag_of_thread'\n",
    "                bag_2 = db.from_sequence([[dir_ + f'/0/output_{thread}.csv', \n",
    "                                          dir_ + f'/0/evolved_{thread}.dat',\n",
    "                                          dir_ + f'/0/logfile_{thread}.dat'] for dir_ in dir_list[:20] for thread in range(30)],\n",
    "                                          npartitions=d) #divisions ?\n",
    "\n",
    "                #Map the preprocessing function to the bag\n",
    "                bag_of_df = bag_2.map(preprocessing_bag_of_thread)\n",
    "                \n",
    "                #TO BE DEFINED\n",
    "                \n",
    "                \n",
    "                #end time\n",
    "                time_f = time.time()\n",
    "\n",
    "                #time difference allocation\n",
    "                time_list[a, d, w, t] = time_f - time_i\n",
    "\n",
    "                #cluster down\n",
    "                cluster.close()\n",
    "            \n",
    "#Save the result\n",
    "#np.save(f'Benchmark_bag.npy', time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c25951-3bbc-48e3-b443-f7c80425d8d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Workers and Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19913c-52e6-484d-b89e-e25fab34eb6c",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_w_thr_old_bag_try.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4ca0fd-e91e-4474-9609-b298a1dc0d62",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7164bbc-7327-417c-b1bb-71b83e71d5ba",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_divisions_old_bag_try.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf2f354-463f-4c0c-9afc-66e8757404ff",
   "metadata": {},
   "source": [
    "### Worker Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957eadd-aa59-452f-aa35-e19ab286e44b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8271f811-8f11-4e04-95c9-ae7fb3fe6e2b",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_dir_old_bag_try.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38ab42-622d-4899-81a5-f45bd462e43f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FG_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0afcf2-afb1-4ccc-b39a-266cf1007789",
   "metadata": {},
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]  |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]  |\n",
    "| Workers option                   | [`Multiprocessing`, `Distributed`] |\n",
    "| Number of Partitions             | [1, 2, 12]  |\n",
    "| Number of Directory              | [1, 2, 3, 4]  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bd94ef-c062-4071-b6b7-429119d55da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = 5\n",
    "workers_list = [1, 2, 4]\n",
    "threads_list = [1, 2, 4]\n",
    "partitions_list = [1, 12, 24]\n",
    "time_list = np.zeros(shape=(attempt, len(partitions_list), len(workers_list), len(threads_list)))\n",
    "multiproccesing_method == False\n",
    "\n",
    "for a in range(attempt):\n",
    "    for d in range(len(partitions_list)):\n",
    "        for w in range(len(workers_list)):\n",
    "            for t in range(len(threads_list)):\n",
    "                print('a:', a, '\\t','d:', partitions_list[d], '\\t', 'w:', workers_list[w], '\\t','t:', threads_list[t])\n",
    "\n",
    "                #cluster up\n",
    "                cluster = SSHCluster(\n",
    "                [\"bhbh-1\", \"bhbh-1\", \"bhbh-2\", \"bhbh-3\", \"bhbh-4\", \"bhbh-5\"],\n",
    "                connect_options={\"client_keys\": \"/home/ubuntu/private/tbertola_key.pem\"},\n",
    "                worker_options={\"n_workers\": workers_list[w],\n",
    "                                \"nthreads\": threads_list[t]}, # because each bhbh-* has 4 cores\n",
    "                scheduler_options={\"port\": 8786, \"dashboard_address\": \":8787\"}\n",
    "                                    )\n",
    "                client=Client(cluster)\n",
    "                \n",
    "                #Configure workers as multiprocessing instead of distributed (default option)\n",
    "                if multiproccesing_method == True:\n",
    "                    dask.config.set({'distributed.worker.multiprocessing-method': 'spawn'})\n",
    "\n",
    "\n",
    "                #begin time\n",
    "                time_i = time.time()\n",
    "\n",
    "                #function loop\n",
    "                n_threads_DEMO = 30\n",
    "                bhbh_list=[]\n",
    "                for dir_name in dir_list[:1]:\n",
    "                    for i in range(n_threads_DEMO):\n",
    "                        _ = dask.delayed(FGpreprocessing_partitions)(dir_name, i, output_column_to_remove, evolved_column_to_remove,\n",
    "                                                          drop_list, n_part=partitions_list[d])\n",
    "                        bhbh_list.append(_)\n",
    "\n",
    "                results = dask.compute(*bhbh_list) \n",
    "                results= dask.compute(*results)\n",
    "\n",
    "                #end time\n",
    "                time_f = time.time()\n",
    "\n",
    "                #time difference allocation\n",
    "                time_list[a, d, w, t] = time_f - time_i\n",
    "\n",
    "                #cluster down\n",
    "                cluster.close()\n",
    "            \n",
    "#Save the result\n",
    "#np.save('Benchmark_normal.npy', time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69464d-5c55-4781-9c1b-28caa86a3d3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Worker and Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6637efa8-68e9-4290-8520-5585967d1bbe",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_w_thr_old.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d9036-8438-4182-9dd8-a085d87680bb",
   "metadata": {},
   "source": [
    "### Worker Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2ea36-2d92-43e2-80a7-cc65880fc3ae",
   "metadata": {},
   "source": [
    "TO BE RUN WITH 4 WORKERS, 4 THREAD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bfadfd-fb7a-436b-87cc-2a59cc986e01",
   "metadata": {},
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65794dbc-3033-41e9-b3b9-50d515405904",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_divisions_old.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3b59c9-83d6-41fb-ac86-5e14decee26d",
   "metadata": {},
   "source": [
    "### Directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088dfcd4-05fe-4a15-9408-f29a6ceb7717",
   "metadata": {},
   "source": [
    "<img src='Figures/bk_dir_old.png' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a4ff8-ad89-420c-a7aa-47a1beef04a2",
   "metadata": {},
   "source": [
    "# Possible improvements and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542963d-ebcc-4cf8-bc46-a07ce04d5900",
   "metadata": {},
   "source": [
    "NFS, reading and writing velocity\n",
    "Distributing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c59439-30f1-4a37-b19e-0152b096afa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d071d296-0f99-43e0-86d7-314b3dd2efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3176d146",
   "metadata": {},
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>\n",
    "    <center>\n",
    "        <div style='  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;'>MAPD-B - Preprocessing of SEVN data for binary black holes mass distribution analysis\n",
    "        </div>\n",
    "    <center>\n",
    "        </br>\n",
    "    <i>Tommaso Bertola, Giacomo Di Prima, Giuseppe Viterbo, Marco Zenari</i></center></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b12757",
   "metadata": {},
   "source": [
    "# Introduction: the computational problem\n",
    "\n",
    "Our aim for this project is to preprocess the data of multiple SEVN simulations of binary systems.\n",
    "\n",
    "SEVN is Python program developed by the Astronomy Department at the University of Padua to simulate the evolution of binary systems. The evolution takes into account different physical phenomena which obey the patterns and laws obeserved in the Universe, especially those seen in the stellar tracks.\n",
    "\n",
    "We will focus our attention specifically to those systems evolving into binary black holes. To study these systems we therefore need to extract from the whole dataset produced by SEVN only some information regarding the initial and final conditions of the binary system evolution.\n",
    "\n",
    "The final goal is to obtain a simple and handy DataFrame listing only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a64acd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data structure\n",
    "\n",
    "To better understading the problem, we will briefely describe the dataset we are given.\n",
    "\n",
    "The dataset consists of a number of folders named after some \"hyperparameters\" given to SEVN while performing the simulations.\n",
    "In our case, we are given 60 different folders, whose names are like `sevn_output_Z0.001A1L1`, `sevn_output_Z0.03A5L1`, ... More specifically the hyperparameters are the numbers following the letters `Z` and `L` which will be included in the final output DataFrame for each record.\n",
    "\n",
    "Inside each folder there are three kinds of files:\n",
    "* `output_{nthread}.csv`\n",
    "* `logfile_{nthread}.dat`\n",
    "* `evolved_{nthread}.dat`\n",
    "\n",
    "where {nthread} is a number ranging from 0 to 29, corresponding to the thread responsible for the computation of those simulations. \n",
    "\n",
    "On average, each of the `output_{nthread}.csv` files occupies 750MB, `logfile_{nthread}.dat` 200MB, and `evolved_{nthread}.dat` 50MB. \n",
    "\n",
    "In total, each folder occupies between 26 to 31GB of data for a gross total of around 1.7TB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab601b",
   "metadata": {},
   "source": [
    "## File schema\n",
    "We describe the schema of the three different kind of files to better explain the following parsing process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ea90d0",
   "metadata": {},
   "source": [
    "### `evolved_{nthread}.dat` schema\n",
    "These files contain the initial properties of the systems that has been successfully evolved by SEVN.\n",
    "These are **fixed width** fields files and therefore we used the `pd.read_table` function to parse them.\n",
    "The fields we are interested in reading are reported below.\n",
    "\n",
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name|string `0_1234....`|Unique in each folder|\n",
    "|Mass_0|float|Initial mass of star 0 (in Solar masses)|\n",
    "|Mass_1|float|Initial mass of star 1 (in Solar masses)|\n",
    "|Z_0|float|Metallicity, same as in directory name (both stars have the same)|\n",
    "|SN_0|string `rapid_gauNS`|Supernova model for star 0|\n",
    "|SN_1|string `rapid_gauNS`|Supernova model for star 1|\n",
    "|a|float| Initial semimajor axis of the binary |\n",
    "|e|float|Initial eccentricity of the binary|\n",
    "                      \n",
    "All other fields in the files are discarded in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6c1e7",
   "metadata": {},
   "source": [
    "### `output_{nthread}.csv` schema\n",
    "The output files contains the final conditions of the simulations.\n",
    "Each row is a different binary system, and each column indicates a different feature of the simulation.\n",
    "The file is formatted in a tipical **csv format**.\n",
    "We report in the following table the columns we are interested in. \n",
    "\n",
    "|Column Name| Data type | Description|\n",
    "|:----|:----|:-----|\n",
    "|name |string `0_1234...`| Unique in each folder|\n",
    "|Mass_0|double| Mass of object 0 (in Solar masses)|\n",
    "|Mass_1|double| Mass of object 1 (in Solar masses)|\n",
    "|RemnantType_0|int|Type of object 0 after evolution|\n",
    "|RemnantType_1|int|Type of object 1 after evolution|\n",
    "|Semimajor|float|Semimajor of the binary (in Solar radius)|\n",
    "|Eccentricity|float| Eccentricity of the binary|\n",
    "|GWtime|float|Gravitational wave orbital decay time|\n",
    "|BWorldtime|int|Time elapsed in the simulations|\n",
    "\n",
    "All other fields in the files are discarded in the analysis.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6e3eea",
   "metadata": {},
   "source": [
    "### `logfile_{nthread}.dat` schema\n",
    "Logfiles are **plain text** files, containing the description of a particular astrophysical event in each row.\n",
    "Each event is univocally associated to a specific binary system evolution by its `name`, however there could be multiple events regarding the same system.\n",
    "To recover the relevant information we have to run different regular expressions.\n",
    "Each regex used specifically captures the type of the event, namely `RLO_BEGIN`, `CE`, or `BSN`. \n",
    "\n",
    "An example of the content of these files is given by the following rows\n",
    "\n",
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>B;0_474492234654248;0;COLLISION;23.667631;0:9.44621:18.0861:3:1:2.61393:1.21104:1:38.822:0.50739:19.1197:10.6772\n",
    "B;0_474492234654248;0;MERGER;23.667631;0:9.446e+00:1.906e+00:0.000e+00:3:0:18.1224:1:2.614e+00:0.000e+00:0.000e+00:1:0:1.21104:12.0601:38.822:0.50739\n",
    "S;0_474492234654248;0;NS;25.471686;5:1.46229:4.64623e+12:63.4538:0.98692\n",
    "S;0_474492234654248;0;SN;25.471686;10.6025:3.06259:1.65091:1.46229:5:2:453.262:345.701:-197.664:-212.483:-187.853\n",
    "S;0_641394535500269;0;WD;32.367569;3.2735:2.57868:1.29894:1.29894:3\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22460388",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Tools\n",
    "To parse the data we leverage on distributed computing techniques. \n",
    "Specifically we extensively use Python Dask library which enable us to manage a cluster of workers hosted on CloudVeneto virtual machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1df023",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Infrastracture configuration\n",
    "![network_configuration.png](network_configuration.png)\n",
    "\n",
    "We will be using 6 different virtual machines, `bhbh-{d,1,2,3,4,5}`. \n",
    "The first, `bhbh-d`, serves as a NFS server, allowing all the other VMs `bhbh-{1,2,3,4,5}` to read, write and crunch the archived data as can be seen from the picture above with the green arrows.\n",
    "Highlighted in orange, all `bhbh-{1..5}` are connected to a Dask cluster, managed solely by `bhbh-1` which performs the action of client, scheduler and worker at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5543746",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up `bhbh-d`\n",
    "After instantiating the VM `bhbh-d` of flavor `cloudveneto.medium`, we attach the volume `bhbh-v` to `/dev/vdb` which is then mounted on `/mnt/bhbh-v`.\n",
    "To allow the other VMs to see the new volume, we use the software provided by `nfs-kernel-server`.\n",
    "\n",
    "The NFS server is configured such that `/mnt/bhbh-v` can be seen by the other VMs and therefore mounted therein.\n",
    "It is important to note that the configuration of the sharing of the volume is made in `sync` mode.\n",
    "The reason being the cluster might potentially alter the consistency of the data if `async` mode were used. \n",
    "\n",
    "Finally, the actual data were copied from the `demoblack.fisica.unipd.it` server to the mounted volume.\n",
    "The operation took a few hours as the total size of the data is around 1.7TB.\n",
    "The operation was carried out via `rsync` utility in order to resume the copying was the network to fail for some unspecified errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b3c48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting up `bhbh-{1,2,3,4,5}`\n",
    "The VMs for the proper data crunching are of the flavour `cloudveneto.large`, having 8GB of RAM each for a total amount of 32GB<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1).\n",
    "The following operations are equally repeated on all the VMs in order to have equally working systems.\n",
    "\n",
    "After installing the `nfs-utils` package, we mount the Network volume issuing the following command `mount -t nfs 10.67.22.169:/mnt/bhbh-v /mnt/bhbh`. \n",
    "For simplicty sake we use `conda` as a package manager. We install the `dask[complete]` suite of packages.\n",
    "This set up is enought to allow the cluster to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b736031",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Particular set up of bhbh-1\n",
    "The `bhbh-1` has some slight differences in the software installed as it is used for different puropses.\n",
    "`bhbh-1` hosts the Jupyter server, acting as a client to the cluster, the cluster scheduler and also a cluster worker.\n",
    "The reason why this set up is used is just for optimizing the memory and computing resources available.\n",
    "A better configuration would have been achieved by moving the client and the scheduler to another VM but this was not easy to do as cloud resources were limited.\n",
    "Indeed, one of the main issues we came across during the tests was a lack of RAM in the `bhbh-1`, presumably due to the exchange of data between workers. For this reason we are discussing different approaches in the following.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) 32GB is comparable to the size of one folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3596e394",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing on the files\n",
    "\n",
    "In this section we are going to give a brief summary of the operations that we have to do on the files for our preprocessing. \n",
    "As anticipated, the final goal is to obtain a dataframe containing only the useful informations and save it for further analysis.\n",
    "\n",
    "In order to obtain only one dataframe we need to read, clean and extract informations from each of the three type of files and finally merge all the informations for each binary in a dataframe.\n",
    "\n",
    "\n",
    "All the preprocessing operations are written inside a function that is then executed in a delayd way with dask. \n",
    "\n",
    "For the preprocessing different attemps have been tried, and in the following section we are going to describe the pros and cons of each of them.\n",
    "\n",
    "Now we take as example one preprocessing function in order to describe the basic operations that we have done. \n",
    "The differences in the preprocessing function of each attempt will be highlighted in the corresponding sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function does the preprocessing\n",
    "# on three files of the same thread:\n",
    "# output_{thread}, evolved_{thread}, logfile_{thread}\n",
    "\n",
    "def preprocessing_bag_of_thread(paths):\n",
    "    '''\n",
    "       paths = python list of the paths of the three files considered [output, evoleved, logfile]\n",
    "    '''\n",
    "    \n",
    "    # list of column names and types to read\n",
    "    \n",
    "    # output_{}.csv\n",
    "    output_column_to_read = ['name', 'Mass_0', 'RemnantType_0',\n",
    "                             'Mass_1', 'RemnantType_1',\n",
    "                             'Semimajor','Eccentricity',\n",
    "                             'GWtime','BWorldtime']\n",
    "    output_column_type = ['string', 'float64', 'int64',\n",
    "                          'float64', 'int64',\n",
    "                          'float64', 'float64',\n",
    "                          'float64', 'float64']\n",
    "\n",
    "    # evolved_{}.dat\n",
    "    evolved_column_to_read = ['name', 'Mass_0',\n",
    "                              'Z_0', 'SN_0',\n",
    "                              'Mass_1', 'SN_1',\n",
    "                              'a', 'e']\n",
    "    evolved_column_type = ['string', 'float64',\n",
    "                           'float64', 'string',\n",
    "                           'float64', 'string',\n",
    "                           'float64', 'float64']\n",
    "    \n",
    "    # further columns to remove at the end \n",
    "    drop_list = ['RemnantType_0',  'RemnantType_1']\n",
    "    \n",
    "   \n",
    "    #OUTPUT files processing\n",
    "    \n",
    "    output = pd.read_csv(paths[0],                              # read the file\n",
    "                         usecols=output_column_to_read,         # read only some cols\n",
    "                         dtype=dict(zip(output_column_to_read,  # specify the types\n",
    "                                        output_column_type))).\\ #\n",
    "                rename(columns={'Mass_0':'Mass_0_out',          # rename columns\n",
    "                                'Mass_1':'Mass_1_out'})         #\n",
    "\n",
    "    # mask to select only the black holes binaries, defined by RemnantType\n",
    "    idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) & (output.Semimajor.notnull())\n",
    "    \n",
    "    # apply the mask\n",
    "    output=output[idxBHBH]\n",
    "        \n",
    "    \n",
    "    #EVOLVED files processing\n",
    "    \n",
    "    #extracting the alpha parameter from the path of the file \n",
    "    alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\",\n",
    "                             paths[1])[0])\n",
    "    \n",
    "    #read the columns we are interested in from the evolved file\n",
    "    evolved = pd.read_table(paths[1],                               # read file\n",
    "                            sep='\\s+',                              # separate by spaces\n",
    "                            usecols=evolved_column_to_read,         # read only some columns\n",
    "                            dtype=dict(zip(evolved_column_to_read,  # specify the types\n",
    "                                           evolved_column_type)))   #\n",
    "    #NB: sep='\\s+' is need because there are different number of spaces separareting the columns\n",
    "    \n",
    "    #adding the column with the alpha parameter\n",
    "    evolved['alpha'] = alpha\n",
    "    \n",
    "    \n",
    "    #LOGFILE files processing\n",
    "    \n",
    "    logfile = pd.read_csv(paths[2],    # read the file\n",
    "                          header=None) # there is no header\n",
    "\n",
    "    \n",
    "    #Running Regex on the line of the logfile to extrac useful informations\n",
    "    df_RLO = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\  # searching for string \"RLO_BEGIN\"\n",
    "                dropna().\\                                                    # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\                          # rename columns\n",
    "                groupby(['name']).\\                                           # grouping by name\n",
    "                size().\\                                                      # and counting the number of RLO\n",
    "                to_frame(name='RLO').\\                                        # \n",
    "                reset_index()                                                 # to have a nice dataframe\n",
    "\n",
    "    \n",
    "    df_CE = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\  # searching for string \"CE\"\n",
    "                dropna().\\                                            # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\                  # rename\n",
    "                groupby(['name']).\\                                   # grouping by name\n",
    "                size().\\                                              # \n",
    "                to_frame(name='CE').\\                                 # and counting the number of CE\n",
    "                reset_index()                                         # to have a nice dataframe\n",
    "    \n",
    "\n",
    "    df_BSN = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\  #searching for string \"BSN\"\n",
    "                dropna().\\                                              # dropping nan\n",
    "                rename(columns={0:'name', 1:'ID'}).\\                    #rename\n",
    "                groupby(['name']).\\                                     #grouping by name\n",
    "                size().\\                                                #\n",
    "                to_frame(name='BSN').\\                                  #and counting the number of BSN\n",
    "                reset_index()                                           #to have a nice dataframe\n",
    "\n",
    "    \n",
    "    #MERGE\n",
    "    bhbh = evolved.merge(output, on=['name'], how='inner').\\  #innerg join on the name between wvolved and output\n",
    "                   merge(df_RLO, on=['name'], how='left').\\   #left join on the name with df_RLO\n",
    "                   merge(df_CE,  on=['name'], how='left').\\   #left join on the name with df_CE\n",
    "                   merge(df_BSN, on=['name'], how='left').\\   #left join on the name with df_BSN\n",
    "                   fillna(value=0).\\                          #setting nan to zero\n",
    "                   drop(columns=drop_list)                    #dropping no longer useful columms\n",
    "    \n",
    "    \n",
    "    #Adding some columns with physical meaning\n",
    "    bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime'] #time delay\n",
    "    \n",
    "    #defining the max mass of output\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                            where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']),\n",
    "                                  other=bhbh['Mass_0_out'])\n",
    "\n",
    "    #defining q=m1/m2 with m2>,m1\n",
    "    bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "    bhbh['q'] = bhbh['q'].\\\n",
    "                where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']),\n",
    "                      other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "    \n",
    "    #defining the Chirp mass\n",
    "    bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))\n",
    "    \n",
    "    return bhbh # return the pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25046c",
   "metadata": {},
   "source": [
    "# Alternative solutions\n",
    "Here we present a summary of the different approaches we tried to tackle the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2466f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing with no optimization - the whole dataset all at once\n",
    "The first attempt consists in loading the whole dataset (1.7TB) into three `dask.dataframe`: respectively all the output.csv, the evolved.dat and the logfile.dat files.\n",
    "\n",
    "This solution is the easiest to implement code wise but it is higly inefficient since it does not take advantage of the hierarchical structure of the data. \n",
    "Because of the cluster's memory limitation, the RAM fills up due to the shuffling operations as soon the merging operations begin.\n",
    "For this reason, this approach does not manage to process the entire volume of data and we have to serialize the operations over smaller batches.\n",
    "\n",
    "The code used for this solution differs from the one previously shown only in the way files are read and the columns on hich the merges are carried out.\n",
    "The files are read all together by leveraging the wildcards capabilities of Dask, instead of using a Pandas DataFrame.\n",
    "The merges are performed on `name`, `Z_0` and `alpha` fileds as these three values uniquely identified the binary systems in the whole dataset.\n",
    "Even varying the number of partitions to higher numbers did not help with the high RAM usage.\n",
    "\n",
    "Note that with the defaults values, Dask would create the following number of partitions for the whole dataset:\n",
    "* partitions for evolved files : 1800\n",
    "* partitions for output files : 21099\n",
    "* partitions for logfile files : 4350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa79914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd  # required to use the dask dataframes\n",
    "\n",
    "# omissis code\n",
    "\n",
    "output = dd.read_csv('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/output_*.csv')   # all files in all directories\n",
    "logfile = dd.read_csv('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/logfile_*.dat', header=None)\n",
    "evolved = dd.read_table('/mnt/bhbh/fiducial_Hrad_5M/sevn_output_*/0/evolved_*.dat', sep='\\s+')\n",
    "\n",
    "# omissis code\n",
    "\n",
    "bhbh = evolved.merge(output, on=['name','Z_0','alpha'], how='inner').\\ # note how the merges are performed\n",
    "               merge(df_RLO, on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               merge(df_CE,  on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               merge(df_BSN, on=['name','Z_0', 'alpha'], how='left').\\\n",
    "               fillna(value=0).\\\n",
    "               drop(columns=drop_list)\n",
    "\n",
    "# omissis code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748a12c",
   "metadata": {},
   "source": [
    "## Folder and thread wise preproccessing\n",
    "In order to reduce even further the computational burden of the merge operations, we exploit the thread-wise organisation of the data by delaying the whole preprocessing over every triplet of files (output, evolved and logfile) in each folder.\n",
    "This is done by appending each task to a list and submitting it to the cluster to compute it.\n",
    "Even this approach fails to process the whole dateset all at once but we noticed it can analyse 300 GB at a time (~10 folders of data).\n",
    "\n",
    "The differences of the preprocessing code run are shown below. \n",
    "More explicitly: data is read folder by folder and thread by thread with dask dataframes.\n",
    "\n",
    "Notice there is an additional argument `n_part` and method `repartition` in the code.\n",
    "They are used only for benchmarking purposes and the first time the function was written we used the default number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bfd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FGpreprocessing_partitions(dir_path: str, n_thread: int,  # notice the explicit signature\n",
    "                               output_column_to_remove: list,\n",
    "                               evolved_column_to_remove: list,\n",
    "                               drop_list: list, n_part: int): \n",
    "    \n",
    "    # file names to read    \n",
    "    output_str = f'{dir_path}/0/output_{n_thread}.csv'    # now we read only one folder at a time\n",
    "    evolved_str = f'{dir_path}/0/evolved_{n_thread}.dat'  #\n",
    "    logfile_str = f'{dir_path}/0/logfile_{n_thread}.dat'  #\n",
    "    \n",
    "   # omitted code    \n",
    "\n",
    "    output = dd.read_csv(output_str).\\\n",
    "                rename(columns={'Mass_0':'Mass_0_out',\n",
    "                                'Mass_1':'Mass_1_out'}).\\\n",
    "                drop(columns=output_column_to_remove).\\\n",
    "                repartition(npartitions = n_part)            # note the possibility to repartition the output files\n",
    "                                                             # it is used only for benchmarking purposes\n",
    "                                                             # it was not used originally\n",
    "                                                             # default number of partition was used instead \n",
    "    \n",
    "    # the following code is identical to the function above\n",
    "    # except for the use of dask dataframes instead of pandas dataframe\n",
    "    '''\n",
    "    dd.read_csv instead of pd.read_csv\n",
    "    '''\n",
    "    # the following code is therefore omitted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47012ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code is to properly map the function to the direcotries and threads\n",
    "\n",
    "n_threads = 30 # the number of thread to analyse\n",
    "bhbh_list=[]   # to store the delayed objects\n",
    "\n",
    "for dir_name in dir_list:         # scan some directories\n",
    "    for i in range(n_threads):    # scan the threads\n",
    "        _ = dask.delayed(FGpreprocessing_partitions)(dir_name, i,              # indexes to point to files\n",
    "                                                     output_column_to_remove,  # stuff to remove\n",
    "                                                     evolved_column_to_remove, #\n",
    "                                                     drop_list)                #\n",
    "        bhbh_list.append(_)       # save in the list\n",
    "        \n",
    "results = dask.compute(*bhbh_list) # trigger the computation\n",
    "results= dask.compute(*results)    # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4319a7e",
   "metadata": {},
   "source": [
    "For this approach we managed to carry out some benchmarks which are presented in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb17b1f",
   "metadata": {},
   "source": [
    "## Repartition by column 'name' the whole dataset 1.7TB - no time wise benefits\n",
    "As we were able to see, the merging operations are quite RAM intensive to carry out, especially for very large chunks of data.\n",
    "According to the documentation, these operations can be sped up by using the dataframe `set_index` method, which explicitly tells dask how to address the data.\n",
    "\n",
    "In this approach we set the column `name` as the index of each dask dataframe.\n",
    "The dataframe contains every file of a specific kind for a a given folder. \n",
    "Then we repartition the dataframes such that each new partition contains only records with the same name.\n",
    "However, while the merge operations actually get faster, the whole setting index and repartitioning processes take so much time that by the end we do not see any benefits, time wise.\n",
    "Therefore we do not carry on with this strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f0dec",
   "metadata": {},
   "source": [
    "## Serialization over the folders and repartion over name - it works but takes 10 hours\n",
    "Taking inspiration from the previous attempt, we leveraged on the data structure, namely the 60 folders.\n",
    "We process one folder at a time in a batch like system developed in a for cycle, processing 30GB at once.\n",
    "This serialization let us avoid processing the whole dataset of 1.7TB at once and reduces the computational weight of operations like `merge`, `reset_index` and `repartition` as they are carried out on fewer data, ~30GB.\n",
    "\n",
    "The whole task takes about 10 hours to fully run and being saved to `parquet` files.\n",
    "Due to the time constraints this approach requires, it is was not benchmarked.\n",
    "\n",
    "The function used has the following form. \n",
    "We highlight only the main differences in the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48017c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FG_new(alpha: float, dir_path: str,      # signature with explicit types\n",
    "           output_column_to_remove: list,    #\n",
    "           evolved_column_to_remove: list,   #\n",
    "           drop_list: list):                 #\n",
    "    \n",
    "    output_str = f'{dir_path}/0/output_*.csv'   # read all threads at once\n",
    "    evolved_str = f'{dir_path}/0/evolved_*.dat' #\n",
    "    logfile_str = f'{dir_path}/0/logfile_*.dat' #\n",
    "\n",
    "    # omitted code to read the files and drop unnecessary columns\n",
    "    \n",
    "    dask_divisions = output.set_index(\"name\").divisions                    # this codes actually performs the best repartition and division splitting\n",
    "    unique_divisions = list(dict.fromkeys(list(dask_divisions)))\n",
    "    \n",
    "    # omitted code\n",
    "    # actually perform the merges and \n",
    "    # the remaining computations\n",
    "    \n",
    "    return bhbh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905b0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code serializes the computation of the while dataset\n",
    "\n",
    "bhbh_list=[]                                                     # the list to store the delayed objects\n",
    "for i,directory in enumerate(dir_list):                          # for all direcotries\n",
    "    bhbh_list.append(client.submit(FG_new,alpha[i], directory,   # submit these computations\n",
    "                                   output_column_to_remove,\n",
    "                                   evolved_column_to_remove,\n",
    "                                   drop_list))\n",
    "\n",
    "for i in range(len(bhbh_list)):  # for each delayed object\n",
    "    bhbh_list[i].compute().to_parquet('/mnt/bhbh/folder_by_folder_repartition/direcotry_'+str(i)+'.parquet') # compute and save its result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a797c5",
   "metadata": {},
   "source": [
    "## Thread wise preproccessing using Dask Bags\n",
    "Still takeing advantage of the thread-wise structure of the data, we change the paradigm of the task: we construct a Dask bag that contains one list for each triplet of threaded files inside the whole dataset. \n",
    "Given one of this lists, each element is the path to a particular kind of file (output, evolved or logfile), with the same thread. \n",
    "We can now apply the preprocessing function, which takes the files' paths as arguments, to each element of the bag and compute a single dataframe as a result by casting the whole bag as a dictionary and than to a dataframe.\n",
    "The benchmarks results are presented in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2e6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_bag_of_thread(paths):\n",
    "    \n",
    "    #lists of columns to read for each file and corresponding type\n",
    "    output_column_to_read = ['name', 'Mass_0', 'RemnantType_0', 'Mass_1', 'RemnantType_1',\n",
    "                         'Semimajor','Eccentricity','GWtime','BWorldtime']\n",
    "\n",
    "    output_column_type = ['string', 'float64', 'int64', 'float64', 'int64',\n",
    "                      'float64', 'float64', 'float64', 'float64']\n",
    "\n",
    "    evolved_column_to_read = ['name', 'Mass_0', 'Z_0', 'SN_0', 'Mass_1', 'SN_1', 'a', 'e']\n",
    "\n",
    "\n",
    "    evolved_column_type = ['string', 'float64', 'float64', 'string', 'float64', \n",
    "                      'string', 'float64', 'float64']\n",
    "\n",
    "    drop_list = ['RemnantType_0',  'RemnantType_1']\n",
    "    \n",
    "   \n",
    "    #Preprocessing OUTPUT\n",
    "    \n",
    "    #reading the file\n",
    "    output = pd.read_csv(paths[0], usecols=output_column_to_read, dtype=dict(zip(output_column_to_read, output_column_type))).\\\n",
    "                rename(columns={'Mass_0':'Mass_0_out', 'Mass_1':'Mass_1_out'})\n",
    "    \n",
    "    #mask to select only the binaries we are interested in\n",
    "    idxBHBH=(output.RemnantType_0==6) & (output.RemnantType_1==6) & (output.Semimajor.notnull())\n",
    "    output=output[idxBHBH]    \n",
    "    \n",
    "    \n",
    "    #preprocessing EVOLVED\n",
    "      \n",
    "    #reading the file\n",
    "    evolved = pd.read_table(paths[1], sep='\\s+', usecols=evolved_column_to_read, dtype=dict(zip(evolved_column_to_read, evolved_column_type)))                \n",
    "    \n",
    "    #extracting alpha with a regex\n",
    "    alpha = float(re.findall(r\".+(?<=A)(.*)(?=L)\", paths[1])[0])\n",
    "    evolved['alpha'] = alpha\n",
    "    \n",
    "    \n",
    "    #preprocessing LOGFILE\n",
    "    \n",
    "    logfile = pd.read_csv(paths[2], header=None)\n",
    "    \n",
    "    \n",
    "    #extracting informations with regex\n",
    "\n",
    "    df_RLO = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);RLO_BEGIN;\").\\\n",
    "                dropna().\\\n",
    "                rename(columns={0:'name', 1:'ID'}).\\\n",
    "                groupby(['name']).\\\n",
    "                size().to_frame(name='RLO').\\\n",
    "                reset_index()\n",
    "\n",
    "    df_CE = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);CE;\").\\\n",
    "                dropna().\\\n",
    "                rename(columns={0:'name', 1:'ID'}).\\\n",
    "                groupby(['name']).\\\n",
    "                size().to_frame(name='CE').\\\n",
    "                reset_index()\n",
    "\n",
    "    df_BSN = logfile[0].str.extract(r\"B;((?:\\d*\\_)?\\d+);(\\d+);BSN;\").\\\n",
    "                dropna().\\\n",
    "                rename(columns={0:'name', 1:'ID'}).\\\n",
    "                groupby(['name']).\\\n",
    "                size().to_frame(name='BSN').\\\n",
    "                reset_index()\n",
    "\n",
    "    \n",
    "    #MERGE\n",
    "    bhbh = evolved.merge(output, on=['name'], how='inner').\\\n",
    "                   merge(df_RLO, on=['name'], how='left').\\\n",
    "                   merge(df_CE,  on=['name'], how='left').\\\n",
    "                   merge(df_BSN, on=['name'], how='left').\\\n",
    "                   fillna(value=0).\\\n",
    "                   drop(columns=drop_list)\n",
    "    \n",
    "    \n",
    "    #add some useful columns\n",
    "    bhbh['tdelay'] = bhbh['GWtime'] + bhbh['BWorldtime']\n",
    "\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_1_out']\n",
    "    bhbh['Mass_max_out'] = bhbh['Mass_max_out'].\\\n",
    "                            where(cond=(bhbh['Mass_max_out'] > bhbh['Mass_0_out']), other=bhbh['Mass_0_out'])\n",
    "\n",
    "    bhbh['q'] = bhbh['Mass_1_out']/bhbh['Mass_0_out']\n",
    "    bhbh['q'] = bhbh['q'].\\\n",
    "                where(cond=(bhbh['Mass_1_out'] < bhbh['Mass_0_out']), other=bhbh['Mass_0_out']/bhbh['Mass_1_out'])\n",
    "\n",
    "    bhbh['Mass_chirp'] = ((bhbh['Mass_0_out'] * bhbh['Mass_1_out'])**(3/5))/((bhbh['Mass_0_out'] + bhbh['Mass_1_out'])**(1/5))\n",
    "    \n",
    "    \n",
    "    return bhbh #a pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfad74f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmarks\n",
    "In order to benchmark our preprocessing functions we decide to vary the number of workers, threads and partitions, testing this variations on a single directory of our dataset for a total of 5 times to obtain meaningfull statistics.\n",
    "Also, we try to see if there is any benefit in using the `multiprocessing` options for the workers, instead of the default `distributed`. We decide to follow a best-result to best-result approach rather than a grid search strategy on the whole parameter space since this latter approach would be extremly time consuming. \n",
    "Firstly we test different couples (`number_of_workers`, `number_of_threads`), letting the partitions be taken automatically. \n",
    "Using the couple of paramteres that took the least amount of time, we assess the effect of the `multiprocessing` option for the workers. \n",
    "Eventually we analyse the impact of varing the number of partitions that the cluster handles. \n",
    "For the data scalability test we evaluate the performance of our best combination of parameters on a varing number of directories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe3e4fd",
   "metadata": {},
   "source": [
    "## FG_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188956df",
   "metadata": {},
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]  |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]  |\n",
    "| Workers option                   | [`Multiprocessing`, `Distributed`] |\n",
    "| Number of Partitions             | TOBEDEFINED |\n",
    "| Number of Directory              | [1, 2, 3, 4]  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc145af",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = 5\n",
    "workers_list = [1, 2, 4]\n",
    "threads_list = [1, 2, 4]\n",
    "partitions_list = [1, 12, 24] \n",
    "time_list = np.zeros(shape=(attempt, len(partitions_list), len(workers_list), len(threads_list)))\n",
    "multiproccesing_method = False\n",
    "\n",
    "for a in range(attempt):\n",
    "    for d in range(len(partitions_list)):\n",
    "        for w in range(len(workers_list)):\n",
    "            for t in range(len(threads_list)):\n",
    "                print('a:', a, '\\t','d:', partitions_list[d], '\\t', 'w:', workers_list[w], '\\t','t:', threads_list[t])\n",
    "\n",
    "                #cluster up\n",
    "                cluster = SSHCluster(\n",
    "                [\"bhbh-1\", \"bhbh-1\", \"bhbh-2\", \"bhbh-3\", \"bhbh-4\", \"bhbh-5\"],\n",
    "                connect_options={\"client_keys\": \"/home/ubuntu/private/tbertola_key.pem\"},\n",
    "                worker_options={\"n_workers\": workers_list[w],\n",
    "                                \"nthreads\": threads_list[t]}, # because each bhbh-* has 4 cores\n",
    "                scheduler_options={\"port\": 8786, \"dashboard_address\": \":8787\"}\n",
    "                                    )\n",
    "                client=Client(cluster)\n",
    "                \n",
    "                #Configure workers as multiprocessing instead of distributed (default option)\n",
    "                if multiproccesing_method == True:\n",
    "                    dask.config.set({'distributed.worker.multiprocessing-method': 'spawn'})\n",
    "\n",
    "                #begin time\n",
    "                time_i = time.time()\n",
    "                \n",
    "                #input bag for the 'preprocessing_bag_of_thread'\n",
    "                bag_2 = db.from_sequence([[dir_ + f'/0/output_{thread}.csv', \n",
    "                                          dir_ + f'/0/evolved_{thread}.dat',\n",
    "                                          dir_ + f'/0/logfile_{thread}.dat'] for dir_ in dir_list[:20] for thread in range(30)],\n",
    "                                          npartitions=d) #divisions ?\n",
    "\n",
    "                #Map the preprocessing function to the bag\n",
    "                bag_of_df = bag_2.map(preprocessing_bag_of_thread)\n",
    "                \n",
    "                #TO BE DEFINED\n",
    "                \n",
    "                \n",
    "                #end time\n",
    "                time_f = time.time()\n",
    "\n",
    "                #time difference allocation\n",
    "                time_list[a, d, w, t] = time_f - time_i\n",
    "\n",
    "                #cluster down\n",
    "                cluster.close()\n",
    "            \n",
    "#Save the result\n",
    "#np.save(f'Benchmark_bag.npy', time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a71435",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Workers and Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75220e6",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_w_thr_old_bag_try.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea261ecd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f3e13",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_divisions_old_bag_try.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc42597",
   "metadata": {},
   "source": [
    "### Worker Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c990c45",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d799af4",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_dir_old_bag_try.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b94dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## FG_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111905c",
   "metadata": {},
   "source": [
    "|Parameter type| Tested values| \n",
    "|:----|:----|\n",
    "| Number of Workers for each  VM   | [1, 2, 4]  |\n",
    "| Number of Threads for each  VM   | [1, 2, 4]  |\n",
    "| Workers option                   | [`Multiprocessing`, `Distributed`] |\n",
    "| Number of Partitions             | [1, 2, 12]  |\n",
    "| Number of Directory              | [1, 2, 3, 4]  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880ee335",
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt = 5\n",
    "workers_list = [1, 2, 4]\n",
    "threads_list = [1, 2, 4]\n",
    "partitions_list = [1, 12, 24]\n",
    "time_list = np.zeros(shape=(attempt, len(partitions_list), len(workers_list), len(threads_list)))\n",
    "multiproccesing_method == False\n",
    "\n",
    "for a in range(attempt):\n",
    "    for d in range(len(partitions_list)):\n",
    "        for w in range(len(workers_list)):\n",
    "            for t in range(len(threads_list)):\n",
    "                print('a:', a, '\\t','d:', partitions_list[d], '\\t', 'w:', workers_list[w], '\\t','t:', threads_list[t])\n",
    "\n",
    "                #cluster up\n",
    "                cluster = SSHCluster(\n",
    "                [\"bhbh-1\", \"bhbh-1\", \"bhbh-2\", \"bhbh-3\", \"bhbh-4\", \"bhbh-5\"],\n",
    "                connect_options={\"client_keys\": \"/home/ubuntu/private/tbertola_key.pem\"},\n",
    "                worker_options={\"n_workers\": workers_list[w],\n",
    "                                \"nthreads\": threads_list[t]}, # because each bhbh-* has 4 cores\n",
    "                scheduler_options={\"port\": 8786, \"dashboard_address\": \":8787\"}\n",
    "                                    )\n",
    "                client=Client(cluster)\n",
    "                \n",
    "                #Configure workers as multiprocessing instead of distributed (default option)\n",
    "                if multiproccesing_method == True:\n",
    "                    dask.config.set({'distributed.worker.multiprocessing-method': 'spawn'})\n",
    "\n",
    "\n",
    "                #begin time\n",
    "                time_i = time.time()\n",
    "\n",
    "                #function loop\n",
    "                n_threads_DEMO = 30\n",
    "                bhbh_list=[]\n",
    "                for dir_name in dir_list[:1]:\n",
    "                    for i in range(n_threads_DEMO):\n",
    "                        _ = dask.delayed(FGpreprocessing_partitions)(dir_name, i, output_column_to_remove, evolved_column_to_remove,\n",
    "                                                          drop_list, n_part=partitions_list[d])\n",
    "                        bhbh_list.append(_)\n",
    "\n",
    "                results = dask.compute(*bhbh_list) \n",
    "                results= dask.compute(*results)\n",
    "\n",
    "                #end time\n",
    "                time_f = time.time()\n",
    "\n",
    "                #time difference allocation\n",
    "                time_list[a, d, w, t] = time_f - time_i\n",
    "\n",
    "                #cluster down\n",
    "                cluster.close()\n",
    "            \n",
    "#Save the result\n",
    "#np.save('Benchmark_normal.npy', time_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfbdb58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Worker and Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4857ea3",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_w_thr_old.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67fc5b9",
   "metadata": {},
   "source": [
    "### Worker Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44359f",
   "metadata": {},
   "source": [
    "TO BE RUN WITH 4 WORKERS, 4 THREAD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfabf3cc",
   "metadata": {},
   "source": [
    "### Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a7ca6",
   "metadata": {},
   "source": [
    "<img src=\"Figures/bk_divisions_old.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b7154",
   "metadata": {},
   "source": [
    "### Directory "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cc2964",
   "metadata": {},
   "source": [
    "<img src='Figures/bk_dir_old.png' width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab69ad",
   "metadata": {},
   "source": [
    "# Possible improvements and limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818947a",
   "metadata": {},
   "source": [
    "NFS, reading and writing velocity\n",
    "Distributing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801ced5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea52ce0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004b989a-e7d3-4f7a-84dc-8348c54bb331",
   "metadata": {},
   "outputs": [],
   "source": [
    "so"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
